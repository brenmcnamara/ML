{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop Derivation\n",
    "\n",
    "This notebook contains a mathematical derivation of the back propagation algorithm.\n",
    "This derivation includes a vectorized implementations and accounts for back propagation\n",
    "over multiple training samples.\n",
    "\n",
    "For a full theoretical deep dive into back propagation, please refer to this article [here](http://neuralnetworksanddeeplearning.com/chap2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediary Error Derivatives\n",
    "\n",
    "The back propagation algorithm can be summarized in 4 equations. The equations utlize an intermediary error derivative which makes the computations of derivatives easier:\n",
    "\n",
    "The intermediary errors are defined as follows:\n",
    "\n",
    "$$\\delta^L_j = \\frac {\\partial C} { \\partial z^L_j } = \\frac {\\partial C} { \\partial a^L_j } \\sigma ' (z^L_j) $$\n",
    "\n",
    "$$\\delta^l_j = \\frac {\\partial C} { \\partial z^l_j } $$\n",
    "\n",
    "* Capital *C* denotes the overall cost function for the network\n",
    "\n",
    "* Capital *L* denotes the last layer of the neural network while lowercase *l* denotes an arbitrary layer\n",
    "\n",
    "* Subscript *j* denotes the jth node in a particular layer, *l*.\n",
    "\n",
    "* $\\odot$ denotes the Hadamard product, which is an element-wise product of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Four Main Equations\n",
    "\n",
    "The four main equations for computing back propagation are used to calculate the partial derivatives of all parameters in the neural network.\n",
    "\n",
    "\n",
    "**1: Error derivative of last layer:**\n",
    "\n",
    "$$\\triangledown C \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "**2: Error derivate of arbitrary layer l:**\n",
    "\n",
    "$$ \\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma '(z^L) $$\n",
    "\n",
    "**3: Partial derivate of cost with respect to bias j in layer l**\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial b^l_j} = \\delta^l_j $$\n",
    "\n",
    "**4: Partial derivate of cost with respect to weight j,k in layer l**\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial w^l_{jk} } = a^{l-1}_k \\delta ^l_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 1\n",
    "\n",
    "$$\\triangledown C \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "By definition, $\\delta^L_j = \\frac { \\partial C } { \\partial z^L_j} = \\frac {\\partial C} { \\partial a^L_j } \\sigma ' (z^L_j) $\n",
    "\n",
    "So $\\delta^L_j = \\sum_k \\frac {\\partial C} {\\partial a_k^L } \\frac { \\partial a_k^L } {\\partial z_j^L} $ where *k* represents the *kth* node in layer L.\n",
    "\n",
    "Note that when $j \\neq k$, the expression $ \\frac { \\partial a_k^L } {\\partial z_j^L} $ becomes 0 since $a_k$ is a function of only $z_k$\n",
    "\n",
    "The above equation then reduces to $\\delta^L_j = \\frac {\\partial C} {\\partial a_k^L } \\frac { \\partial a_k^L } {\\partial z_j^L} $\n",
    "\n",
    "We can vectorize this equation as follows: $\\delta^L = (\\delta_1^L, \\delta_2^L, ..., \\delta_{n_L}^L) $ where $n_L$ is the number of nodes in layer *L*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 2\n",
    "\n",
    "$$\\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "By definition, $\\delta^l_j = \\frac { \\partial C } { \\partial z^l_j } $\n",
    "\n",
    "Using induction, we can derive: $ \\delta^l_j = \\sum_k \\delta^{l+1}_k \\frac {\\partial z^{l+1}_k} {\\partial z^l_j} $\n",
    "\n",
    "Note that $z_j^{l+1} = (w_j^{l+1})^T a^l + b_j^{l+1}$ where $w_j^{l+1}$ is a vector containing the *jth* row of weight matrix $W^{l+1}$. These are all the parameters feeding into the *jth* node in layer $l + 1$.\n",
    "\n",
    "Also note that $\\frac {\\partial a_j^l} {\\partial z_k^l}$ is 0 when $k \\neq j$, otherwise it is the derivative of our activation function, $\\sigma '(z_j^l)$.\n",
    "\n",
    "Putting this all together, we get $\\frac {\\partial z_k^{l+1}} {\\partial z_j^l} = W^{l+1}_{jk} \\sigma '(z_j^l)$\n",
    "\n",
    "And $ \\delta_j^{l} = \\sum_k \\delta_k^{l+1} W^{l+1}_{jk} \\sigma '(z_k^l) $\n",
    "\n",
    "This expression can be vectorized to $\\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^L) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 3\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial b^l_j} = \\delta^l_j $$\n",
    "\n",
    "By definition, $\\delta^l_j = \\frac { \\partial C } { \\partial z^l_j } $\n",
    "\n",
    "And by definition, $z_j^l = (w_j^l)^T a^l + b_j^l$ where $w_j^l$ is a vector containing the *jth* row of weight matrix $W^l$\n",
    "\n",
    "So $\\frac { \\partial z^l_j } { \\partial b^l_j } = 1$\n",
    "\n",
    "And $\\frac { \\partial C } { \\partial b^l_j } = \\delta^l_j \\frac { \\partial z^l_j } { \\partial b^l_j } = \\delta^l_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 4\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial w^l_{jk} } = a^{l-1}_k \\delta ^l_j$$\n",
    "\n",
    "By definition, $\\delta^l_j = \\frac { \\partial C } { \\partial z^l_j } $\n",
    "\n",
    "And by definition, $z_j^l = (w_j^l)^T a^l + b_j^l$ where $w_j^l$ is a vector containing the *jth* row of weight matrix $W^l$\n",
    "\n",
    "So $\\frac { \\partial z^l_j } { \\partial w^l_{jk} } = a^{l-1}_k$\n",
    "\n",
    "And $\\frac { \\partial C } { \\partial w^l_{jk} } = \\delta^l_j \\frac { \\partial z^l_j } { \\partial w^l_{jk} } = \\delta^l_j *a^{l-1}_k $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the Equations\n",
    "\n",
    "We want the equations to be vectorized so that computing them across many samples and many parameters can happen using as few loops / math operations as possible.\n",
    "\n",
    "We can further vectorize equations 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Equation 3\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial b^l_j} = \\delta^l_j $$\n",
    "\n",
    "There isn't much extra work to do here, all we have to note is that we can simultaneously update all biases in a layer by using the error derivative directly, since the error derivative is equivalent to a vector of all biases in a single layer.\n",
    "\n",
    "$$ \\frac {\\triangledown C} {\\partial b^l} = \\delta^l $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Equation 4\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial w^l_{jk} } = a^{l-1}_k \\delta ^l_j$$\n",
    "\n",
    "We want to create a gradient to update the weights in a layer that has the same dimensions as the weight matrix: $ \\mathbb{R}^{n_l \\times n_{l-1}}$.\n",
    "\n",
    "This can be accomplished using an outer product of the vectors $\\delta^l \\in \\mathbb{R}^{n_l} $ and $a^{l-1} \\in \\mathbb{R}^{n_{l-1}}$:\n",
    "\n",
    "$$\\frac {\\triangledown C} {\\partial W^l} = \\delta^l (a^{l-1})^T$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
