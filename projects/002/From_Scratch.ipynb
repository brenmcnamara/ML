{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Gaussian Curve\n",
    "\n",
    "In the notebook implementing the basic back propagation algorithm from scratch, I used as an example case of learning a gaussian curve. In theory, our model should be able to learn this well since there is no noise in the model (we are learning a deterministic function).\n",
    "\n",
    "I will explore more robust approaches in deep learning to properly learn this gaussian curve. Some ideas I will try:\n",
    "\n",
    "- Use more data\n",
    "- Hyper-Parameter Testing\n",
    "  - Number and Shape of Layers in the Network\n",
    "  - Learning Rate\n",
    "- Better Training Procedure: will use the Adam Algorithm instead of SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Gaussian Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_curve(mean, variance, delta=0.2):\n",
    "    std = math.sqrt(variance)\n",
    "    normalize = 1 / (std * math.sqrt(2 * 3.14159))\n",
    "\n",
    "    def gaussian_curve(x, y):\n",
    "        term = (x - mean) / std\n",
    "        expected = normalize * np.exp(-1/2 * term * term)\n",
    "        return 1 if abs(expected - y) <= delta else 0\n",
    "    \n",
    "    return gaussian_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want the number of negative samples to overwhelm the\n",
    "# number of positive examples since this can adversly affect the\n",
    "# ability of the model to learn. Will downsample the dominant\n",
    "# class.\n",
    "\n",
    "# Note: c is a param that reduces the harshness of downsampling.\n",
    "# When c == 1.0, downsampling approximately equalizes the positive\n",
    "# and negative samples. As c gets larger, the dominant class\n",
    "# is downsampled less. In the limit, c will perform no downsampling.\n",
    "def downsample(points, labels, c=1.0):\n",
    "    pos_mask = labels == 1\n",
    "    neg_mask = labels == 0\n",
    "\n",
    "    pos_count = np.sum(pos_mask)\n",
    "    neg_count = np.sum(neg_mask)\n",
    "    \n",
    "    # Note: If values are inequal, gauranteed that one of the\n",
    "    # following ratios will be 1.0 after clipping, which means\n",
    "    # that the minority class won't get downsampled.\n",
    "    neg_to_pos = min(1.0, neg_count / float(pos_count)) ** (1/c)\n",
    "    pos_to_neg = min(1.0, pos_count / float(neg_count)) ** (1/c)\n",
    "    \n",
    "    random_values = np.random.rand(labels.shape[0])\n",
    "    \n",
    "    pos_keep_mask = (random_values < neg_to_pos) * pos_mask\n",
    "    neg_keep_mask = (random_values < pos_to_neg) * neg_mask\n",
    "    \n",
    "    keep_mask = pos_keep_mask + neg_keep_mask\n",
    "    \n",
    "    return (points[:, keep_mask], labels[keep_mask])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_func = create_gaussian_curve(0.0, 0.05)\n",
    "points = np.random.rand(2, num_points) * 4 - 2\n",
    "labels = np.apply_along_axis(lambda x: gauss_func(x[0], x[1]), axis=0, arr=points)\n",
    "\n",
    "points, labels = downsample(points, labels, c=2)\n",
    "n = points.shape[1]\n",
    "\n",
    "random_assignment = np.random.rand(n)\n",
    "train_mask = random_assignment <= 0.8\n",
    "valid_mask = (random_assignment > 0.8) & (random_assignment <= 0.9)\n",
    "test_mask = random_assignment > 0.9\n",
    "\n",
    "train_data = points[:, train_mask]\n",
    "train_labels = labels[train_mask]\n",
    "\n",
    "valid_data = points[:, valid_mask]\n",
    "valid_labels = labels[valid_mask]\n",
    "\n",
    "test_data = points[:, test_mask]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's make a plot to see where the curve is being drawn.\n",
    "# We will look at the first 5,000 points.\n",
    "\n",
    "for i in range(5000):\n",
    "    point = train_data[:, i]\n",
    "    label = train_labels[i]\n",
    "    plt.plot(point[0], point[1], 'go' if label == 1 else 'ro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(X0):\n",
    "    \"\"\"\n",
    "    Generates a normalize and un-normalize function, which can be attached\n",
    "    to a model. The normalization parameters are based on the training data.\n",
    "    The model will automatically do the work of normalizing the data before\n",
    "    it is fed into the network.\n",
    "    \n",
    "    X0 - The data used to normalize values. The data is of shape d x m, where\n",
    "         d is the number of features and m is the number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X0.shape[1]\n",
    "    mean = np.sum(X0, axis=1).reshape(-1, 1) / m\n",
    "    variance = np.sum(X0 * X0, axis=1).reshape(-1, 1) / m\n",
    "    std = np.sqrt(variance)\n",
    "\n",
    "    def normalize(X):\n",
    "        return (X - mean) / std\n",
    "\n",
    "    return normalize\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    mask = X >= 0\n",
    "    return X * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deriv(X):\n",
    "    return (X >= 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    overflow_mask = X >= 700\n",
    "    return 1. / (1 + np.exp(-X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(X):\n",
    "    sig = sigmoid(X)\n",
    "    return sig * (1 - sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output, expected):\n",
    "    m = output.shape[1]\n",
    "\n",
    "    # Offset the output if there are any values exactly equal to\n",
    "    # zero or one to avoid log(0).\n",
    "    zero_correct = (output == 0).astype(float) * 1e-10 \n",
    "    one_correct = (output == 1).astype(float) * (-1e-10)\n",
    "    output = output + zero_correct + one_correct\n",
    "\n",
    "    return np.sum((expected * -np.log(output)) + (1 - expected) * -np.log(1 - output)) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss_grad(output, expected):\n",
    "\n",
    "    m = output.shape[1]\n",
    "    \n",
    "    # Want to avoid divide by 0.\n",
    "    epsilon = 1e-10\n",
    "    zero_correct = (output == 0.0).astype(float) * epsilon\n",
    "    one_correct = (output == 1.0).astype(float) * -epsilon\n",
    "    output = output + zero_correct + one_correct\n",
    "    \n",
    "    return (1. / m) * ((expected / output) - (1 - expected) / (1 - output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When initializing the layers, we will assume that the output\n",
    "# layer is always a single output through a sigmoid activation.\n",
    "def xavier_initialization(input_size, layer_sizes):\n",
    "    layers = []\n",
    "    \n",
    "    prev_layer_size = input_size\n",
    "\n",
    "    for size in layer_sizes:\n",
    "        normalize = 1. / math.sqrt(prev_layer_size)\n",
    "        weights = np.random.randn(size, prev_layer_size) * normalize\n",
    "        biases = np.random.randn(size, 1) * normalize\n",
    "        layers.append((weights, biases))\n",
    "\n",
    "        prev_layer_size = size\n",
    "        \n",
    "    # Add a final output layer for sigmoid activation.\n",
    "    weights = np.random.randn(1, prev_layer_size)\n",
    "    biases = np.random.randn(1, 1)\n",
    "    layers.append((weights, biases))\n",
    "    \n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X_raw):\n",
    "    X = model['normalize'](X_raw)\n",
    "    Y = X\n",
    "\n",
    "    linear_outputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for (i, (weights, biases)) in enumerate(model['layers'][0:-1]):\n",
    "        Y = np.dot(weights, Y) + biases\n",
    "        linear_outputs.append(Y)\n",
    "\n",
    "        Y = relu(Y)\n",
    "        outputs.append(Y)\n",
    "        \n",
    "\n",
    "    # Note: Last layer is processed by sigmoid activation.\n",
    "    weights, biases = model['layers'][-1]\n",
    "\n",
    "    Y = np.dot(weights, Y) + biases\n",
    "    linear_outputs.append(Y)\n",
    "\n",
    "    Y = sigmoid(Y)\n",
    "    outputs.append(Y)\n",
    "\n",
    "    # Save the results of the forward pass so we can do a backward\n",
    "    # pass on them later.\n",
    "    if 'no_grad_check' not in model or not model['no_grad_check']:\n",
    "        model['linear_outputs'] = linear_outputs\n",
    "        model['input'] = X\n",
    "        model['outputs'] = outputs\n",
    "        model['result'] = Y\n",
    "\n",
    "    return Y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    Y_hat = forward(model, X)\n",
    "    return (Y_hat >= 0.5).astype(float).reshape(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, expected):\n",
    "    # Note: We are assuming the model has already gone through a\n",
    "    # forward pass.\n",
    "\n",
    "    layers = model['layers']\n",
    "    linear_outputs = model['linear_outputs']\n",
    "    outputs = model['outputs']\n",
    "    X = model['input']\n",
    "    result = model['result']\n",
    "\n",
    "    L = len(layers)\n",
    "    m = result.shape[1] # Number of samples.\n",
    "\n",
    "    # Note: We can have multiple samples in the outputs, so we\n",
    "    # will end up with a gradient per sample.\n",
    "    output_prev_layer = X if len(outputs) <= 1 else outputs[-2]\n",
    "    loss_grad = binary_cross_entropy_loss_grad(result, expected)\n",
    "    error_last_layer = loss_grad * sigmoid_deriv(linear_outputs[-1])\n",
    "    bias_grad_last_layer = np.sum(error_last_layer, axis=1).reshape(-1, 1) / m\n",
    "    weights_grad_last_layer = np.dot(error_last_layer, output_prev_layer.T) / m # outer product\n",
    "    grad_last_layer = (weights_grad_last_layer, bias_grad_last_layer)\n",
    "\n",
    "    errors = [error_last_layer]\n",
    "    grads = [grad_last_layer]\n",
    "\n",
    "    # Enumerate layers in reverse order to compute errors\n",
    "    # and gradients.\n",
    "    for i in range(L - 2, -1, -1):\n",
    "        linear_output = linear_outputs[i]\n",
    "        output_prev_layer = X if i == 0 else outputs[i-1]\n",
    "        error_next_layer = errors[-1]\n",
    "        weights_next_layer, bias_next_layer = layers[i+1]\n",
    "\n",
    "        error = np.dot(weights_next_layer.T, error_next_layer) * sigmoid_deriv(linear_output)\n",
    "        bias_grad = np.sum(error, axis=1).reshape(-1, 1) / m\n",
    "        weights_grad = np.dot(error, output_prev_layer.T) / m\n",
    "        \n",
    "        errors.append(error)\n",
    "        grads.append((weights_grad, bias_grad))\n",
    "        \n",
    "    # Reverse the order of errors and gradients so they go from\n",
    "    # first layer to last.\n",
    "    errors.reverse()\n",
    "    grads.reverse()\n",
    "    \n",
    "    if 'no_grad_check' not in model or not model['no_grad_check']:\n",
    "        model['errors'] = errors\n",
    "        model['grads'] = grads\n",
    "    \n",
    "    return grads, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(model, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Returns True if the gradient was clipped, False otherwise.\n",
    "    \"\"\"\n",
    "    l2_norm = 0\n",
    "\n",
    "    for grad in model['grads']:\n",
    "        grad_weight, grad_bias = grad\n",
    "        l2_norm += np.sum(grad_weight * grad_weight)\n",
    "        l2_norm += np.sum(grad_bias * grad_bias)\n",
    "        \n",
    "    l2_norm = math.sqrt(l2_norm)\n",
    "\n",
    "    if l2_norm <= threshold:\n",
    "        return False\n",
    "\n",
    "    for (i, grad) in enumerate(model['grads']):\n",
    "        grad_weight, grad_bias = grad\n",
    "\n",
    "        grad_weight = grad_weight * threshold / l2_norm\n",
    "        grad_bias = grad_bias * threshold / l2_norm\n",
    "\n",
    "        model['grads'][i] = (grad_weight, grad_bias)\n",
    "        \n",
    "    return True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_off(model):\n",
    "    model['no_grad_check'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_on(model):\n",
    "    model['no_grad_check'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_SGD(model, epoch, lr=0.1, lr_decay=0.0):\n",
    "    # Note: We are assuming the model has already gone\n",
    "    # through back propagation and all the gradients have\n",
    "    # been calculated.\n",
    "    \n",
    "    new_layers = []\n",
    "\n",
    "    lr = lr / (1 + lr_decay * epoch)\n",
    "\n",
    "    for (i, layer) in enumerate(model['layers']):\n",
    "        weight, bias = layer\n",
    "        grad_weight, grad_bias = model['grads'][i]\n",
    "        \n",
    "        weight = weight + (lr * grad_weight)\n",
    "        bias = bias + (lr * grad_bias)\n",
    "\n",
    "        model['layers'][i] = (weight, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_Adam(model):\n",
    "    # We keep track of exponential moving averages of our\n",
    "    # Adam optimization algorithm, cached in the model.\n",
    "    # If we are starting to train from scratch, we need\n",
    "    # to know to set these averages to 0.\n",
    "    mt = []\n",
    "    vt = []\n",
    "\n",
    "    for (i, layers) in enumerate(model['layers']):\n",
    "        weight, bias = layers\n",
    "        mt.append((np.zeros_like(weight), np.zeros_like(bias)))\n",
    "        vt.append((np.zeros_like(weight), np.zeros_like(bias)))\n",
    "        \n",
    "    model['mt'] = mt\n",
    "    model['vt'] = vt\n",
    "    model['t'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_Adam(model, lr, beta_1=0.9, beta_2=0.999, epsilon=1e-30):\n",
    "    # Note: We are assuming the model has already gone through\n",
    "    # back propagation and all the gradients have been calculated.\n",
    "    \n",
    "    t = model['t'] + 1\n",
    "\n",
    "    one_minus_beta_1 = 1 - beta_1\n",
    "    one_minus_beta_2 = 1 - beta_2\n",
    "\n",
    "    one_minus_beta_1t = 1 - math.pow(beta_1, t)\n",
    "    one_minus_beta_2t = 1 - math.pow(beta_2, t)\n",
    "\n",
    "    for (i, layer) in enumerate(model['layers']):\n",
    "        weight, bias = layer\n",
    "        grad_weight, grad_bias = model['grads'][i]\n",
    "\n",
    "        mt_minus_1_weight, mt_minus_1_bias = model['mt'][i]\n",
    "        vt_minus_1_weight, vt_minus_1_bias = model['vt'][i]\n",
    "        \n",
    "        # Calculating exponentially moving averages.\n",
    "        mt_weight = beta_1 * mt_minus_1_weight + one_minus_beta_1 * grad_weight\n",
    "        mt_bias = beta_1 * mt_minus_1_bias + one_minus_beta_1 * grad_bias\n",
    "        \n",
    "        vt_weight = beta_2 * vt_minus_1_weight + one_minus_beta_2 * grad_weight * grad_weight\n",
    "        vt_bias = beta_2 * vt_minus_1_bias + one_minus_beta_2 * grad_bias * grad_bias\n",
    "\n",
    "        # Bias correction of our moving averages.\n",
    "        mt_hat_weight = mt_weight / one_minus_beta_1t\n",
    "        mt_hat_bias = mt_bias / one_minus_beta_1t\n",
    "        vt_hat_weight = vt_weight / one_minus_beta_2t\n",
    "        vt_hat_bias = vt_bias / one_minus_beta_2t\n",
    "\n",
    "        # Calculate change in weights.\n",
    "        deltat_weight = mt_hat_weight / (np.sqrt(vt_hat_weight) + epsilon)\n",
    "        deltat_bias = mt_hat_bias / (np.sqrt(vt_hat_bias) + epsilon)\n",
    "        \n",
    "        # Update weights.\n",
    "        weight = weight + (lr * deltat_weight)\n",
    "        bias = bias + (lr * deltat_bias)\n",
    "\n",
    "        # Write to model.\n",
    "        model['layers'][i] = (weight, bias)\n",
    "        model['mt'][i] = (mt_weight, mt_bias)\n",
    "        model['vt'][i] = (vt_weight, vt_bias)\n",
    "\n",
    "    \n",
    "    model['t'] = t\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_mag(model):\n",
    "    n_params = 0\n",
    "    weight_mag = 0\n",
    "\n",
    "    for weight, bias in model['layers']:\n",
    "        n_params += np.prod(weight.shape)\n",
    "        n_params += np.prod(bias.shape)\n",
    "        \n",
    "        weight_mag += np.sum(weight * weight)\n",
    "        weight_mag += np.sum(bias * bias)\n",
    "        \n",
    "    weight_mag = math.sqrt(weight_mag) / n_params\n",
    "\n",
    "    return weight_mag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_mag(model):\n",
    "    n_params = 0\n",
    "    grad_mag = 0\n",
    "    \n",
    "    for grad_weight, grad_bias in model['grads']:\n",
    "        n_params += np.prod(grad_weight.shape)\n",
    "        n_params += np.prod(grad_bias.shape)\n",
    "        \n",
    "        grad_mag += np.sum(grad_weight * grad_weight)\n",
    "        grad_mag += np.sum(grad_bias * grad_bias)\n",
    "        \n",
    "    grad_mag = math.sqrt(grad_mag) / n_params\n",
    "    \n",
    "    return grad_mag\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(model, X, Y):\n",
    "    Y_hat = predict(model, X)\n",
    "\n",
    "    tp = np.sum(((Y_hat == 1) & (Y == 1)).astype(float))\n",
    "    tn = np.sum(((Y_hat == 0) & (Y == 0)).astype(float))\n",
    "    fp = np.sum(((Y_hat == 1) & (Y == 0)).astype(float))\n",
    "    fn = np.sum(((Y_hat == 0) & (Y == 1)).astype(float))\n",
    "\n",
    "    # Rows are predictions, Columns are ground truth.\n",
    "    return np.array([[tn, fn], [fp, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(model, X, Y):\n",
    "    matrix = calculate_confusion_matrix(model, X, Y)\n",
    "    tn = matrix[0, 0]\n",
    "    fn = matrix[0, 1]\n",
    "    fp = matrix[1, 0]\n",
    "    tp = matrix[1, 1]\n",
    "    \n",
    "    precision = float(tp) / (tp + fn)\n",
    "    recall = float(tp) / (tp + fp)\n",
    "    \n",
    "    return (precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, Y):\n",
    "    Y_predicted = predict(model, X)\n",
    "    m = Y.shape[0]\n",
    "    return np.sum(Y_predicted == Y).astype(float) / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_sizes, normalize):\n",
    "    model = {\n",
    "        'layers': xavier_initialization(2, layer_sizes),\n",
    "        'normalize': normalize,\n",
    "    }\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, lr, batch_size, step_strategy, sgd_lr_decay, epochs=2000, logs=True):\n",
    "\n",
    "    m = train_data.shape[1]\n",
    "\n",
    "    train_errors = []\n",
    "    grad_mags = []\n",
    "    weight_mags = []\n",
    "\n",
    "    reset_Adam(model)\n",
    "\n",
    "    if step_strategy == 'Adam':\n",
    "        step_update = lambda model: step_Adam(model, lr=lr)\n",
    "    else:\n",
    "        step_update = lambda model: step_SGD(model, lr=lr, lr_decay=sgd_lr_decay)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for (batch_i, batch_start) in enumerate(range(0, m, batch_size)):\n",
    "            batch_X = train_data[:, batch_start:min(m, batch_start+batch_size)]\n",
    "            batch_Y = train_labels[batch_start:min(m, batch_start+batch_size)]\n",
    "\n",
    "            forward(model, batch_X)\n",
    "            backward(model, batch_Y)\n",
    "            # gradient_clipping(model, threshold=1.0)\n",
    "            step_update(model)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # Turn off gradient checking while we calculate training\n",
    "            # and test loss.\n",
    "            grad_off(model)\n",
    "\n",
    "            train_output = forward(model, train_data)\n",
    "            train_error = binary_cross_entropy_loss(train_output, train_labels)\n",
    "            train_errors.append(train_error)\n",
    "\n",
    "            grad_mag = calculate_grad_mag(model)\n",
    "            weight_mag = calculate_weight_mag(model)\n",
    "\n",
    "            grad_mags.append(grad_mag)\n",
    "            weight_mags.append(weight_mag)\n",
    "\n",
    "            if logs and i % 100 == 0:\n",
    "                print(f'Epoch {i + 1}')\n",
    "                print(f'Train Error {train_error}')\n",
    "                print(f'Weight Mag={weight_mag} Grad Mag={grad_mag}')                \n",
    "                print('')\n",
    "\n",
    "            # Turn gradient checking back on before we start a new epoch.\n",
    "            grad_on(model)\n",
    "\n",
    "    return train_errors, weight_mags, grad_mags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 / 4\n",
      "Model = Adam Optimizer\n",
      "Epoch 1\n",
      "Train Error 1.9055506945064289\n",
      "Weight Mag=0.003322327257032688 Grad Mag=4.503223739407166e-07\n",
      "\n",
      "Epoch 101\n",
      "Train Error 0.4155638096790631\n",
      "Weight Mag=0.004109751727898958 Grad Mag=6.749582814126967e-08\n",
      "\n",
      "Epoch 201\n",
      "Train Error 0.3316815259374453\n",
      "Weight Mag=0.005469788008202655 Grad Mag=2.1134706877347412e-07\n",
      "\n",
      "Epoch 301\n",
      "Train Error 0.16371003069327456\n",
      "Weight Mag=0.006464943295757421 Grad Mag=7.903607587063649e-08\n",
      "\n",
      "Epoch 401\n",
      "Train Error 0.13689648643755367\n",
      "Weight Mag=0.007234490523694303 Grad Mag=9.299841275308819e-08\n",
      "\n",
      "Epoch 501\n",
      "Train Error 0.12367196976777577\n",
      "Weight Mag=0.008033924311775461 Grad Mag=9.297675319771465e-08\n",
      "\n",
      "Epoch 601\n",
      "Train Error 0.1374080506942088\n",
      "Weight Mag=0.008748776709853873 Grad Mag=1.4083245295046536e-07\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-62f7689e1b9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0msgd_lr_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sgd_lr_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_mags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_mags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_lr_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd_lr_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mgrad_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-ae21be11c13c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, train_labels, lr, batch_size, step_strategy, sgd_lr_decay, epochs, logs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# gradient_clipping(model, threshold=1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mstep_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-13bc156dbc3f>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(model, expected)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_next_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_next_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbias_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mweights_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prev_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "valid_errors = []\n",
    "train_error_logs = []\n",
    "grad_mag_logs = []\n",
    "weight_mag_logs = []\n",
    "train_times = []\n",
    "\n",
    "data = train_data\n",
    "labels = train_labels\n",
    "\n",
    "hyperparams = [\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [80, 40, 8],\n",
    "        'model_name': 'Adam Optimizer',\n",
    "        'sgd_lr_decay': None,\n",
    "        'step_strategy': 'Adam',\n",
    "    },\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [80, 40, 8],\n",
    "        'model_name': 'SGD Optimizer - No Decay',\n",
    "        'sgd_lr_decay': 0.0,\n",
    "        'step_strategy': 'SGD',\n",
    "    },\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [80, 40, 8],\n",
    "        'model_name': 'SGD Optimizer - Small Decay',\n",
    "        'sgd_lr_decay': 0.2,\n",
    "        'step_strategy': 'SGD',\n",
    "    },\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [80, 40, 8],\n",
    "        'model_name': 'SGD Optimizer - Large Decay',\n",
    "        'sgd_lr_decay': 1.0,\n",
    "        'step_strategy': 'SGD',\n",
    "    },\n",
    "]\n",
    "\n",
    "# For each set of hyper parameters, create and train a new model.\n",
    "# Record the trained model with the corresponding validation error.\n",
    "for (i, hyperparam) in enumerate(hyperparams):\n",
    "    print(f'Training model {i + 1} / {len(hyperparams)}')\n",
    "    print(f'Model = {hyperparam[\"model_name\"]}')\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    normalize = normalizer(data)\n",
    "    model = create_model(hyperparam['layer_sizes'], normalize)\n",
    "\n",
    "    # Pull out hyperparams\n",
    "    lr = hyperparam['lr']\n",
    "    batch_size = hyperparam['batch_size']\n",
    "    step_strategy = hyperparam['step_strategy']\n",
    "    sgd_lr_decay = hyperparam['sgd_lr_decay']\n",
    "\n",
    "    train_errors, weight_mags, grad_mags = train_model(model, data, labels, lr=lr, batch_size=batch_size, step_strategy=step_strategy, sgd_lr_decay=sgd_lr_decay)\n",
    "\n",
    "    grad_off(model)\n",
    "\n",
    "    train_error_logs.append(train_errors)\n",
    "    weight_mag_logs.append(weight_mags)\n",
    "    grad_mag_logs.append(grad_mags)\n",
    "\n",
    "    valid_output = forward(model, valid_data)\n",
    "    valid_error = binary_cross_entropy_loss(valid_output, valid_labels)\n",
    "\n",
    "    grad_on(model)\n",
    "    \n",
    "    models.append(model)\n",
    "    valid_errors.append(valid_error)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    print(f'Model took {(end_time - start_time) / 60:0.2f}m to train.')\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, model) in enumerate(models):\n",
    "    line = plt.plot(train_error_logs[i])\n",
    "\n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad_mags in grad_mag_logs:\n",
    "    plt.plot(grad_mags)\n",
    "    \n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_mags in weight_mag_logs:\n",
    "    plt.plot(weight_mags)\n",
    "    \n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with lowest validation error.\n",
    "best_model_index = np.argmin(valid_errors)\n",
    "best_model = models[best_model_index]\n",
    "best_accuracy = calculate_accuracy(best_model, valid_data, valid_labels)\n",
    "\n",
    "f'Model {best_model_index + 1} with accuracy {best_accuracy:.03f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = calculate_precision_recall(best_model, valid_data, valid_labels)\n",
    "\n",
    "f'Precision: {precision}, Recall: {recall}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out predictions on test set.\n",
    "\n",
    "test_predictions = predict(best_model, test_data)\n",
    "\n",
    "for i in range(test_data.shape[1]): \n",
    "    point = test_data[:, i]\n",
    "    predicted_label = test_predictions[i]\n",
    "    # label = test_labels[i]\n",
    "    plt.plot(point[0], point[1], 'go' if predicted_label == 1 else 'ro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
