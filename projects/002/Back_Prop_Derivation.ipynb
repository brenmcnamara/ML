{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop Derivation\n",
    "\n",
    "This notebook contains a mathematical derivation of the back propagation algorithm.\n",
    "This derivation includes a vectorized implementations and accounts for back propagation\n",
    "over multiple training samples.\n",
    "\n",
    "For a full theoretical deep dive into back propagation, please refer to this article [here](http://neuralnetworksanddeeplearning.com/chap2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediary Errors\n",
    "\n",
    "The back propagation algorithm can be summarized in 4 equations. The equations utlize an intermediary error derivative which makes the computations of derivatives easier:\n",
    "\n",
    "The intermediary errors are defined as follows:\n",
    "\n",
    "$$\\delta^L_j = \\frac {\\partial C} { \\partial a^L_j } \\sigma ' (z^L_j) $$\n",
    "\n",
    "$$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "* Capital *C* denotes the overall cost function for the network\n",
    "\n",
    "* Capital *L* denotes the last layer of the neural network while lowercase *l* denotes an arbitrary layer\n",
    "\n",
    "* Subscript *j* denotes the jth node in a particular layer, *l*.\n",
    "\n",
    "* $\\odot$ denotes the Hadamard product, which is an element-wise product of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Four Main Equations\n",
    "\n",
    "The four main equations for computing back propagation are used to calculate the partial derivatives of all parameters in the neural network.\n",
    "\n",
    "\n",
    "**1: Error derivative of last layer:**\n",
    "\n",
    "$$\\triangledown C \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "**2: Error derivate of arbitrary layer l:**\n",
    "\n",
    "$$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "**3: Partial derivate of cost with respect to bias j in layer l**\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial b^l_j} = \\delta^l_j $$\n",
    "\n",
    "**4: Partial derivate of cost with respect to weight j,k in layer l**\n",
    "\n",
    "$$ \\frac {\\partial C} {\\partial w^l_{jk} } = a^{l-1}_k \\delta ^l_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 1\n",
    "\n",
    "$$\\triangledown C \\odot \\sigma ' (z^L) $$\n",
    "\n",
    "By definition, $\\delta^L_j = \\frac {\\partial C} { \\partial a^L_j } \\sigma ' (z^L_j$\n",
    "\n",
    "So $\\delta^L_j = \\sum_k \\frac {\\partial C} {\\partial a_k^L } \\frac { \\partial a_k^L } {\\partial z_j^L} $ where *k* represents the *kth* node in layer L.\n",
    "\n",
    "Note that when $j \\neq k$, the expression $ \\frac { \\partial a_k^L } {\\partial z_j^L} $ becomes 0 since $a_k$ is a function of only $z_k$\n",
    "\n",
    "The above equation then reduces to $\\delta^L_j = \\frac {\\partial C} {\\partial a_k^L } \\frac { \\partial a_k^L } {\\partial z_j^L} $\n",
    "\n",
    "We can vectorize this equation as follows: $\\delta^L = (\\delta_1^L, \\delta_2^L, ..., \\delta_{n_L}^L) $ where $n_L$ is the number of nodes in layer *L*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Equation 2\n",
    "\n",
    "$$\\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma ' (z^L) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
