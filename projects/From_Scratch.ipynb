{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Gaussian Curve\n",
    "\n",
    "In the notebook implementing the basic back propagation algorithm from scratch, I used as an example case of learning a gaussian curve. In theory, our model should be able to learn this well since there is no noise in the model (we are learning a deterministic function).\n",
    "\n",
    "I will explore more robust approaches in deep learning to properly learn this gaussian curve. Some ideas I will try:\n",
    "\n",
    "- Use more data\n",
    "- Hyper-Parameter Testing\n",
    "  - Number and Shape of Layers in the Network\n",
    "  - Learning Rate\n",
    "- Better Training Procedure: will use the Adam Algorithm instead of SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Gaussian Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_curve(mean, variance, delta=0.2):\n",
    "    std = math.sqrt(variance)\n",
    "    normalize = 1 / (std * math.sqrt(2 * 3.14159))\n",
    "\n",
    "    def gaussian_curve(x, y):\n",
    "        term = (x - mean) / std\n",
    "        expected = normalize * np.exp(-1/2 * term * term)\n",
    "        return 1 if abs(expected - y) <= delta else 0\n",
    "    \n",
    "    return gaussian_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want the number of negative samples to overwhelm the\n",
    "# number of positive examples since this can adversly affect the\n",
    "# ability of the model to learn. Will downsample the dominant\n",
    "# class.\n",
    "\n",
    "# Note: c is a param that reduces the harshness of downsampling.\n",
    "# When c == 1.0, downsampling approximately equalizes the positive\n",
    "# and negative samples. As c gets larger, the dominant class\n",
    "# is downsampled less. In the limit, c will perform no downsampling.\n",
    "def downsample(points, labels, c=1.0):\n",
    "    pos_mask = labels == 1\n",
    "    neg_mask = labels == 0\n",
    "\n",
    "    pos_count = np.sum(pos_mask)\n",
    "    neg_count = np.sum(neg_mask)\n",
    "    \n",
    "    # Note: If values are inequal, gauranteed that one of the\n",
    "    # following ratios will be 1.0 after clipping, which means\n",
    "    # that the minority class won't get downsampled.\n",
    "    neg_to_pos = min(1.0, neg_count / float(pos_count)) ** (1/c)\n",
    "    pos_to_neg = min(1.0, pos_count / float(neg_count)) ** (1/c)\n",
    "    \n",
    "    random_values = np.random.rand(labels.shape[0])\n",
    "    \n",
    "    pos_keep_mask = (random_values < neg_to_pos) * pos_mask\n",
    "    neg_keep_mask = (random_values < pos_to_neg) * neg_mask\n",
    "    \n",
    "    keep_mask = pos_keep_mask + neg_keep_mask\n",
    "    \n",
    "    return (points[:, keep_mask], labels[keep_mask])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_func = create_gaussian_curve(0.0, 0.05)\n",
    "points = np.random.rand(2, num_points) * 4 - 2\n",
    "labels = np.apply_along_axis(lambda x: gauss_func(x[0], x[1]), axis=0, arr=points)\n",
    "\n",
    "random_assignment = np.random.rand(num_points)\n",
    "train_mask = random_assignment <= 0.8\n",
    "valid_mask = (random_assignment > 0.8) & (random_assignment <= 0.9)\n",
    "test_mask = random_assignment > 0.9\n",
    "\n",
    "train_data = points[:, train_mask]\n",
    "train_labels = labels[train_mask]\n",
    "\n",
    "valid_data = points[:, valid_mask]\n",
    "valid_labels = labels[valid_mask]\n",
    "\n",
    "test_data = points[:, test_mask]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's make a plot to see where the curve is being drawn.\n",
    "# We will look at the first 5,000 points.\n",
    "\n",
    "for i in range(5000):\n",
    "    point = train_data[:, i]\n",
    "    label = train_labels[i]\n",
    "    plt.plot(point[0], point[1], 'go' if label == 1 else 'ro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(X0):\n",
    "    \"\"\"\n",
    "    Generates a normalize and un-normalize function, which can be attached\n",
    "    to a model. The normalization parameters are based on the training data.\n",
    "    The model will automatically do the work of normalizing the data before\n",
    "    it is fed into the network.\n",
    "    \n",
    "    X0 - The data used to normalize values. The data is of shape d x m, where\n",
    "         d is the number of features and m is the number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X0.shape[1]\n",
    "    mean = np.sum(X0, axis=1).reshape(-1, 1) / m\n",
    "    variance = np.sum(X0 * X0, axis=1).reshape(-1, 1) / m\n",
    "    std = np.sqrt(variance)\n",
    "\n",
    "    def normalize(X):\n",
    "        return (X - mean) / std\n",
    "\n",
    "    return normalize\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    mask = X >= 0\n",
    "    return X * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deriv(X):\n",
    "    return (X >= 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    overflow_mask = X >= 700\n",
    "    return 1. / (1 + np.exp(-X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(X):\n",
    "    sig = sigmoid(X)\n",
    "    return sig * (1 - sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output, expected):\n",
    "    m = output.shape[1]\n",
    "\n",
    "    # Offset the output if there are any values exactly equal to\n",
    "    # zero or one to avoid log(0).\n",
    "    zero_correct = (output == 0).astype(float) * 1e-10 \n",
    "    one_correct = (output == 1).astype(float) * (-1e-10)\n",
    "    output = output + zero_correct + one_correct\n",
    "\n",
    "    return np.sum((expected * -np.log(output)) + (1 - expected) * -np.log(1 - output)) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss_grad(output, expected):\n",
    "\n",
    "    m = output.shape[1]\n",
    "    \n",
    "    # Want to avoid divide by 0.\n",
    "    epsilon = 1e-10\n",
    "    zero_correct = (output == 0.0).astype(float) * epsilon\n",
    "    one_correct = (output == 1.0).astype(float) * -epsilon\n",
    "    output = output + zero_correct + one_correct\n",
    "    \n",
    "    return (1. / m) * ((expected / output) - (1 - expected) / (1 - output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When initializing the layers, we will assume that the output\n",
    "# layer is always a single output through a sigmoid activation.\n",
    "def xavier_initialization(input_size, layer_sizes):\n",
    "    layers = []\n",
    "    \n",
    "    prev_layer_size = input_size\n",
    "\n",
    "    for size in layer_sizes:\n",
    "        normalize = 1. / math.sqrt(prev_layer_size)\n",
    "        weights = np.random.randn(size, prev_layer_size) * normalize\n",
    "        biases = np.random.randn(size, 1) * normalize\n",
    "        layers.append((weights, biases))\n",
    "\n",
    "        prev_layer_size = size\n",
    "        \n",
    "    # Add a final output layer for sigmoid activation.\n",
    "    weights = np.random.randn(1, prev_layer_size)\n",
    "    biases = np.random.randn(1, 1)\n",
    "    layers.append((weights, biases))\n",
    "    \n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X_raw):\n",
    "    X = model['normalize'](X_raw)\n",
    "    Y = X\n",
    "\n",
    "    linear_outputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for (i, (weights, biases)) in enumerate(model['layers'][0:-1]):\n",
    "        Y = np.dot(weights, Y) + biases\n",
    "        linear_outputs.append(Y)\n",
    "\n",
    "        Y = relu(Y)\n",
    "        outputs.append(Y)\n",
    "        \n",
    "\n",
    "    # Note: Last layer is processed by sigmoid activation.\n",
    "    weights, biases = model['layers'][-1]\n",
    "\n",
    "    Y = np.dot(weights, Y) + biases\n",
    "    linear_outputs.append(Y)\n",
    "\n",
    "    Y = sigmoid(Y)\n",
    "    outputs.append(Y)\n",
    "\n",
    "    # Save the results of the forward pass so we can do a backward\n",
    "    # pass on them later.\n",
    "    if 'no_grad_check' not in model or not model['no_grad_check']:\n",
    "        model['linear_outputs'] = linear_outputs\n",
    "        model['input'] = X\n",
    "        model['outputs'] = outputs\n",
    "        model['result'] = Y\n",
    "\n",
    "    return Y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    Y_hat = forward(model, X)\n",
    "    return (Y_hat >= 0.5).astype(float).reshape(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, expected):\n",
    "    # Note: We are assuming the model has already gone through a\n",
    "    # forward pass.\n",
    "\n",
    "    layers = model['layers']\n",
    "    linear_outputs = model['linear_outputs']\n",
    "    outputs = model['outputs']\n",
    "    X = model['input']\n",
    "    result = model['result']\n",
    "\n",
    "    L = len(layers)\n",
    "    m = result.shape[1] # Number of samples.\n",
    "\n",
    "    # Note: We can have multiple samples in the outputs, so we\n",
    "    # will end up with a gradient per sample.\n",
    "    output_prev_layer = X if len(outputs) <= 1 else outputs[-2]\n",
    "    loss_grad = binary_cross_entropy_loss_grad(result, expected)\n",
    "    error_last_layer = loss_grad * sigmoid_deriv(linear_outputs[-1])\n",
    "    bias_grad_last_layer = np.sum(error_last_layer, axis=1).reshape(-1, 1) / m\n",
    "    weights_grad_last_layer = np.dot(error_last_layer, output_prev_layer.T) / m # outer product\n",
    "    grad_last_layer = (weights_grad_last_layer, bias_grad_last_layer)\n",
    "\n",
    "    errors = [error_last_layer]\n",
    "    grads = [grad_last_layer]\n",
    "\n",
    "    # Enumerate layers in reverse order to compute errors\n",
    "    # and gradients.\n",
    "    for i in range(L - 2, -1, -1):\n",
    "        linear_output = linear_outputs[i]\n",
    "        output_prev_layer = X if i == 0 else outputs[i-1]\n",
    "        error_next_layer = errors[-1]\n",
    "        weights_next_layer, bias_next_layer = layers[i+1]\n",
    "\n",
    "        error = np.dot(weights_next_layer.T, error_next_layer) * sigmoid_deriv(linear_output)\n",
    "        bias_grad = np.sum(error, axis=1).reshape(-1, 1) / m\n",
    "        weights_grad = np.dot(error, output_prev_layer.T) / m\n",
    "        \n",
    "        errors.append(error)\n",
    "        grads.append((weights_grad, bias_grad))\n",
    "        \n",
    "    # Reverse the order of errors and gradients so they go from\n",
    "    # first layer to last.\n",
    "    errors.reverse()\n",
    "    grads.reverse()\n",
    "    \n",
    "    if 'no_grad_check' not in model or not model['no_grad_check']:\n",
    "        model['errors'] = errors\n",
    "        model['grads'] = grads\n",
    "    \n",
    "    return grads, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(model, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Returns True if the gradient was clipped, False otherwise.\n",
    "    \"\"\"\n",
    "    l2_norm = 0\n",
    "\n",
    "    for grad in model['grads']:\n",
    "        grad_weight, grad_bias = grad\n",
    "        l2_norm += np.sum(grad_weight * grad_weight)\n",
    "        l2_norm += np.sum(grad_bias * grad_bias)\n",
    "        \n",
    "    l2_norm = math.sqrt(l2_norm)\n",
    "\n",
    "    if l2_norm <= threshold:\n",
    "        return False\n",
    "\n",
    "    for (i, grad) in enumerate(model['grads']):\n",
    "        grad_weight, grad_bias = grad\n",
    "\n",
    "        grad_weight = grad_weight * threshold / l2_norm\n",
    "        grad_bias = grad_bias * threshold / l2_norm\n",
    "\n",
    "        model['grads'][i] = (grad_weight, grad_bias)\n",
    "        \n",
    "    return True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_off(model):\n",
    "    model['no_grad_check'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_on(model):\n",
    "    model['no_grad_check'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_SGD(model, lr=0.1):\n",
    "    # Note: We are assuming the model has already gone\n",
    "    # through back propagation and all the gradients have\n",
    "    # been calculated.\n",
    "    \n",
    "    new_layers = []\n",
    "\n",
    "    for (i, layer) in enumerate(model['layers']):\n",
    "        weight, bias = layer\n",
    "        grad_weight, grad_bias = model['grads'][i]\n",
    "        \n",
    "        weight = weight + (lr * grad_weight)\n",
    "        bias = bias + (lr * grad_bias)\n",
    "\n",
    "        model['layers'][i] = (weight, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_Adam(model):\n",
    "    # We keep track of exponential moving averages of our\n",
    "    # Adam optimization algorithm, cached in the model.\n",
    "    # If we are starting to train from scratch, we need\n",
    "    # to know to set these averages to 0.\n",
    "    mt = []\n",
    "    vt = []\n",
    "\n",
    "    for (i, layers) in enumerate(model['layers']):\n",
    "        weight, bias = layers\n",
    "        mt.append((np.zeros_like(weight), np.zeros_like(bias)))\n",
    "        vt.append((np.zeros_like(weight), np.zeros_like(bias)))\n",
    "        \n",
    "    model['mt'] = mt\n",
    "    model['vt'] = vt\n",
    "    model['t'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_Adam(model, lr, beta_1=0.9, beta_2=0.999, epsilon=1e-30):\n",
    "    # Note: We are assuming the model has already gone through\n",
    "    # back propagation and all the gradients have been calculated.\n",
    "    \n",
    "    t = model['t'] + 1\n",
    "\n",
    "    one_minus_beta_1 = 1 - beta_1\n",
    "    one_minus_beta_2 = 1 - beta_2\n",
    "\n",
    "    one_minus_beta_1t = 1 - math.pow(beta_1, t)\n",
    "    one_minus_beta_2t = 1 - math.pow(beta_2, t)\n",
    "\n",
    "    for (i, layer) in enumerate(model['layers']):\n",
    "        weight, bias = layer\n",
    "        grad_weight, grad_bias = model['grads'][i]\n",
    "\n",
    "        mt_minus_1_weight, mt_minus_1_bias = model['mt'][i]\n",
    "        vt_minus_1_weight, vt_minus_1_bias = model['vt'][i]\n",
    "        \n",
    "        # Calculating exponentially moving averages.\n",
    "        mt_weight = beta_1 * mt_minus_1_weight + one_minus_beta_1 * grad_weight\n",
    "        mt_bias = beta_1 * mt_minus_1_bias + one_minus_beta_1 * grad_bias\n",
    "        \n",
    "        vt_weight = beta_2 * vt_minus_1_weight + one_minus_beta_2 * grad_weight * grad_weight\n",
    "        vt_bias = beta_2 * vt_minus_1_bias + one_minus_beta_2 * grad_bias * grad_bias\n",
    "\n",
    "        # Bias correction of our moving averages.\n",
    "        mt_hat_weight = mt_weight / one_minus_beta_1t\n",
    "        mt_hat_bias = mt_bias / one_minus_beta_1t\n",
    "        vt_hat_weight = vt_weight / one_minus_beta_2t\n",
    "        vt_hat_bias = vt_bias / one_minus_beta_2t\n",
    "\n",
    "        # Calculate change in weights.\n",
    "        deltat_weight = mt_hat_weight / (np.sqrt(vt_hat_weight) + epsilon)\n",
    "        deltat_bias = mt_hat_bias / (np.sqrt(vt_hat_bias) + epsilon)\n",
    "        \n",
    "        # Update weights.\n",
    "        weight = weight + (lr * deltat_weight)\n",
    "        bias = bias + (lr * deltat_bias)\n",
    "\n",
    "        # Write to model.\n",
    "        model['layers'][i] = (weight, bias)\n",
    "        model['mt'][i] = (mt_weight, mt_bias)\n",
    "        model['vt'][i] = (vt_weight, vt_bias)\n",
    "\n",
    "    \n",
    "    model['t'] = t\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_mag(model):\n",
    "    n_params = 0\n",
    "    weight_mag = 0\n",
    "\n",
    "    for weight, bias in model['layers']:\n",
    "        n_params += np.prod(weight.shape)\n",
    "        n_params += np.prod(bias.shape)\n",
    "        \n",
    "        weight_mag += np.sum(weight * weight)\n",
    "        weight_mag += np.sum(bias * bias)\n",
    "        \n",
    "    weight_mag = math.sqrt(weight_mag) / n_params\n",
    "\n",
    "    return weight_mag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_mag(model):\n",
    "    n_params = 0\n",
    "    grad_mag = 0\n",
    "    \n",
    "    for grad_weight, grad_bias in model['grads']:\n",
    "        n_params += np.prod(grad_weight.shape)\n",
    "        n_params += np.prod(grad_bias.shape)\n",
    "        \n",
    "        grad_mag += np.sum(grad_weight * grad_weight)\n",
    "        grad_mag += np.sum(grad_bias * grad_bias)\n",
    "        \n",
    "    grad_mag = math.sqrt(grad_mag) / n_params\n",
    "    \n",
    "    return grad_mag\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(model, X, Y):\n",
    "    Y_hat = predict(model, X)\n",
    "\n",
    "    tp = np.sum(((Y_hat == 1) & (Y == 1)).astype(float))\n",
    "    tn = np.sum(((Y_hat == 0) & (Y == 0)).astype(float))\n",
    "    fp = np.sum(((Y_hat == 1) & (Y == 0)).astype(float))\n",
    "    fn = np.sum(((Y_hat == 0) & (Y == 1)).astype(float))\n",
    "\n",
    "    # Rows are predictions, Columns are ground truth.\n",
    "    return np.array([[tn, fn], [fp, tp]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(model, X, Y):\n",
    "    matrix = calculate_confusion_matrix(model, X, Y)\n",
    "    tn = matrix[0, 0]\n",
    "    fn = matrix[0, 1]\n",
    "    fp = matrix[1, 0]\n",
    "    tp = matrix[1, 1]\n",
    "    \n",
    "    precision = float(tp) / (tp + fn)\n",
    "    recall = float(tp) / (tp + fp)\n",
    "    \n",
    "    return (precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, Y):\n",
    "    Y_predicted = predict(model, X)\n",
    "    m = Y.shape[0]\n",
    "    return np.sum(Y_predicted == Y).astype(float) / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_sizes, normalize):\n",
    "    model = {\n",
    "        'layers': xavier_initialization(2, layer_sizes),\n",
    "        'normalize': normalize,\n",
    "    }\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, lr, batch_size, epochs=10000, logs=True):\n",
    "\n",
    "    m = train_data.shape[1]\n",
    "\n",
    "    train_errors = []\n",
    "    grad_mags = []\n",
    "    weight_mags = []\n",
    "\n",
    "    reset_Adam(model)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for (batch_i, batch_start) in enumerate(range(0, m, batch_size)):\n",
    "            batch_X = train_data[:, batch_start:min(m, batch_start+batch_size)]\n",
    "            batch_Y = train_labels[batch_start:min(m, batch_start+batch_size)]\n",
    "\n",
    "            forward(model, batch_X)\n",
    "            backward(model, batch_Y)\n",
    "            # gradient_clipping(model, threshold=1.0)\n",
    "            step_Adam(model, lr=lr)\n",
    "            # step_SGD(model, lr=lr)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # Turn off gradient checking while we calculate training\n",
    "            # and test loss.\n",
    "            grad_off(model)\n",
    "\n",
    "            train_output = forward(model, train_data)\n",
    "            train_error = binary_cross_entropy_loss(train_output, train_labels)\n",
    "            train_errors.append(train_error)\n",
    "\n",
    "            grad_mag = calculate_grad_mag(model)\n",
    "            weight_mag = calculate_weight_mag(model)\n",
    "\n",
    "            grad_mags.append(grad_mag)\n",
    "            weight_mags.append(weight_mag)\n",
    "\n",
    "            if logs and i % 100 == 0:\n",
    "                print(f'Epoch {i + 1}')\n",
    "                print(f'Train Error {train_error}')\n",
    "                print(f'Weight Mag={weight_mag} Grad Mag={grad_mag}')                \n",
    "                print('')\n",
    "\n",
    "            # Turn gradient checking back on before we start a new epoch.\n",
    "            grad_on(model)\n",
    "\n",
    "    return train_errors, weight_mags, grad_mags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 / 3\n",
      "lr=1e-05, layer_sizes=[50, 20, 8], batch_size=1000\n",
      "Epoch 1\n",
      "Train Error 1.2067541752202833\n",
      "Weight Mag=0.007891016548018654 Grad Mag=8.644038397645724e-06\n",
      "\n",
      "Epoch 101\n",
      "Train Error 0.3031240717588715\n",
      "Weight Mag=0.00797258830685276 Grad Mag=3.5744743821900417e-07\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8417331886ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer_sizes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_mags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_mags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgrad_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-696853536b5a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, train_labels, lr, batch_size, epochs, logs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mbatch_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# gradient_clipping(model, threshold=1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f25735e48760>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X_raw)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlinear_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "valid_errors = []\n",
    "train_error_logs = []\n",
    "grad_mag_logs = []\n",
    "weight_mag_logs = []\n",
    "train_times = []\n",
    "\n",
    "data = train_data\n",
    "labels = train_labels\n",
    "\n",
    "hyperparams = [\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [50, 20, 8],\n",
    "    },\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [80, 40, 8],\n",
    "    },\n",
    "    {\n",
    "        'batch_size': 1000,\n",
    "        'lr': 0.00001,\n",
    "        'layer_sizes': [100, 60, 20, 8],\n",
    "    },\n",
    "]\n",
    "\n",
    "# For each set of hyper parameters, create and train a new model.\n",
    "# Record the trained model with the corresponding validation error.\n",
    "for (i, hyperparam) in enumerate(hyperparams):\n",
    "    print(f'Training model {i + 1} / {len(hyperparams)}')\n",
    "    print(f'lr={hyperparam[\"lr\"]}, layer_sizes={hyperparam[\"layer_sizes\"]}, batch_size={hyperparam[\"batch_size\"]}')\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    normalize = normalizer(data)\n",
    "    model = create_model(hyperparam['layer_sizes'], normalize)\n",
    "\n",
    "    train_errors, weight_mags, grad_mags = train_model(model, data, labels, lr=hyperparam['lr'], batch_size=hyperparam['batch_size'])\n",
    "\n",
    "    grad_off(model)\n",
    "\n",
    "    train_error_logs.append(train_errors)\n",
    "    weight_mag_logs.append(weight_mags)\n",
    "    grad_mag_logs.append(grad_mags)\n",
    "\n",
    "    valid_output = forward(model, valid_data)\n",
    "    valid_error = binary_cross_entropy_loss(valid_output, valid_labels)\n",
    "\n",
    "    grad_on(model)\n",
    "    \n",
    "    models.append(model)\n",
    "    valid_errors.append(valid_error)\n",
    "\n",
    "    end_time = time()\n",
    "\n",
    "    print(f'Model took {(end_time - start_time) / 60:0.2f}m to train.')\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, model) in enumerate(models):\n",
    "    line = plt.plot(train_error_logs[i])\n",
    "\n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad_mags in grad_mag_logs:\n",
    "    plt.plot(grad_mags)\n",
    "    \n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_mags in weight_mag_logs:\n",
    "    plt.plot(weight_mags)\n",
    "    \n",
    "plt.legend([i + 1 for i in range(len(models))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with lowest validation error.\n",
    "best_model_index = np.argmin(valid_errors)\n",
    "best_model = models[best_model_index]\n",
    "best_accuracy = calculate_accuracy(best_model, valid_data, valid_labels)\n",
    "\n",
    "f'Model {best_model_index + 1} with accuracy {best_accuracy:.03f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = calculate_precision_recall(best_model, valid_data, valid_labels)\n",
    "\n",
    "f'Precision: {precision}, Recall: {recall}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out predictions on test set.\n",
    "\n",
    "test_predictions = predict(best_model, test_data)\n",
    "\n",
    "for i in range(test_data.shape[1]): \n",
    "    point = test_data[:, i]\n",
    "    predicted_label = test_predictions[i]\n",
    "    # label = test_labels[i]\n",
    "    plt.plot(point[0], point[1], 'go' if predicted_label == 1 else 'ro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
