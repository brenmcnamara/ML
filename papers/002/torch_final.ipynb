{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "Note that the purpose of this notebook is to put together a final implementation of all the data-fetching, pre-processing, training, and evaluation of the work in the paper. There will not be much discussion on the decisions made for hyper-parameters, configurations, and other implementation details. Please look at *torch_exmplore* for an exploration on some of these implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Fetching and Pre-Processing\n",
    "\n",
    "1. Convert samples into tokens\n",
    "2. Remove punctuation-only tokens\n",
    "3. Sub-sample tokens in the vocabulary\n",
    "4. Marginalize all tokens ocurring less than 5 times in the corpus\n",
    "5. Learn phrases in the vocabulary\n",
    "6. Create training examples for negative sampling (input word + set of skip-gram output words)\n",
    "7. One-hot encode the training examples (create encoder / decoder pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../../data/language-modeling-benchmark-r13/'\n",
    "PATH_TRAINING_CORPUS = os.path.join(PATH_DATA, 'training-monolingual.tokenized.shuffled')\n",
    "PATH_HOLDOUT_CORPUS = os.path.join(PATH_DATA, 'heldout-monolingual.tokenized.shuffled')\n",
    "\n",
    "CONTEXT_SIZE = 5\n",
    "CORPUS_FILE_COUNT = 10                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punctuation_token(token):\n",
    "    \"\"\"\n",
    "    A token is a punctuation token if the characters consist of\n",
    "    only punctuation characters\n",
    "    \"\"\"\n",
    "    return len(token) == len([c for c in token if c in punc_list])\n",
    "\n",
    "\n",
    "def get_sample_filenames():\n",
    "    \"\"\"\n",
    "    Get the names of the files to be sampled from.\n",
    "    \"\"\"\n",
    "    return os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "\n",
    "def get_raw_samples(filepath):\n",
    "    \"\"\"\n",
    "    Fetch all raw samples from a particular file path. This will pull\n",
    "    the data from the files and tokenize the samples. No pre-processing\n",
    "    is done here.\n",
    "    \n",
    "    filepath - The filepath\n",
    "    \"\"\"\n",
    "    raw_samples = []\n",
    "\n",
    "    with open(filename) as file:\n",
    "        data = file.read().split(\"\\n\")\n",
    "\n",
    "        for sentence in data:\n",
    "            raw_samples.append([t for t in word_tokenize(sentence.lower())])\n",
    "\n",
    "    return raw_samples\n",
    "\n",
    "\n",
    "def create_unigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Given a set of samples, generate a set of unigram counts\n",
    "    within those samples.\n",
    "    \"\"\"\n",
    "    uc = {}\n",
    "    for sample in samples:\n",
    "        for token in sample:\n",
    "            if token not in uc:\n",
    "                uc[token] = 0\n",
    "            uc[token] = uc[token] + 1\n",
    "    \n",
    "    return uc\n",
    "   \n",
    "\n",
    "def create_unigram_and_bigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Given a set of samples, generate a set of unigram and\n",
    "    bigram counts within those samples.\n",
    "    \"\"\"\n",
    "    uc = {}\n",
    "    bc = {}\n",
    "    for sample in samples:\n",
    "        for i in range(len(sample)):\n",
    "            token = sample[i]\n",
    "            if token not in uc:\n",
    "                uc[token] = 0\n",
    "            uc[token] = uc[token] + 1\n",
    "            \n",
    "        for i in range(0, len(sample), 2):\n",
    "            if i + 1 >= len(sample):\n",
    "                break\n",
    "            \n",
    "            bigram = (sample[i], sample[i+1])\n",
    "            if bigram not in bc:\n",
    "                bc[bigram] = 0\n",
    "            bc[bigram] = bc[bigram] + 1\n",
    "        \n",
    "    return (uc, bc)\n",
    "\n",
    "\n",
    "def remove_pred(samples, pred):\n",
    "    \"\"\"\n",
    "    Given a predicate, remove a particular token from the\n",
    "    sample being processed. If there is at most 1 token in\n",
    "    the sample after removing the tokens, then the entire sample\n",
    "    is also removed.\n",
    "    \n",
    "    samples - The samples to remove from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    pred - A predicate that returns True if the token should be\n",
    "           kept, False otherwise.\n",
    "    \"\"\"\n",
    "    new_samples = []\n",
    "    for sample in samples:\n",
    "        new_sample = []\n",
    "\n",
    "        for token in sample:\n",
    "            if pred(token):\n",
    "                new_sample.append(token)\n",
    "                \n",
    "        if len(new_sample) > 1:\n",
    "            new_samples.append(new_sample)\n",
    "    \n",
    "\n",
    "def remove_tokens(samples, tokens):\n",
    "    \"\"\"\n",
    "    Given a set of tokens, remove those tokens from the list of samples.\n",
    "    If a token exists in the set of tokens, it is removed from the set\n",
    "    of samples.\n",
    "    \n",
    "    samples - The samples to remove from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    tokens - Tokens to remove from all samples.\n",
    "    \"\"\"\n",
    "    return remove_pred(samples, lambda t: t not in tokens)\n",
    "\n",
    "\n",
    "def perform_subsampling(samples, uc):\n",
    "    \"\"\"\n",
    "    Perform sub-sampling according to the procedure outlined in the paper.\n",
    "    \n",
    "    samples - The samples to sub-sample from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    uc - The unigram counts of words.\n",
    "    \"\"\"\n",
    "    #  This is the t value used in the paper. Note that it is not\n",
    "    # independent of corpus size.\n",
    "    t = 10e-5\n",
    "\n",
    "    tokens = uc.keys()\n",
    "    counts = np.array(list(uc.values()))\n",
    "    total_count = np.sum(counts)\n",
    "    frequencies = counts / total_count\n",
    "    p_values = np.maximum(0, 1 - np.sqrt(t / frequencies))\n",
    "    \n",
    "    # TODO: HERE I AM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([False, True], replace=False, p=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-21-572577a258a8>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-572577a258a8>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    total_time = time.time() - start_time\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "FILE_NAMES = os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for filename in FILE_NAMES:\n",
    "    with open(filename) as file:\n",
    "        \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Network\n",
    "\n",
    "1. Create a pytorch model for processing the data.\n",
    "2. Define the negative sampling criterion.\n",
    "3. Create a training procedure for running samples through network\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating the Embeddings\n",
    "\n",
    "1. Load the sets of analogies from the datasets\n",
    "2. Find matching tokens for the analogies and discard analogies without analogous tokens\n",
    "2. Evaluate model against analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
