{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "Note that the purpose of this notebook is to put together a final implementation of all the data-fetching, pre-processing, training, and evaluation of the work in the paper. There will not be much discussion on the decisions made for hyper-parameters, configurations, and other implementation details. Please look at *torch_exmplore* for an exploration on some of these implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Fetching and Pre-Processing\n",
    "\n",
    "1. Convert samples into tokens\n",
    "2. Remove punctuation-only tokens\n",
    "3. Sub-sample tokens in the vocabulary\n",
    "4. Marginalize all tokens ocurring less than 5 times in the corpus\n",
    "5. Learn phrases in the vocabulary\n",
    "6. Create training examples for negative sampling (input word + set of skip-gram output words)\n",
    "7. One-hot encode the training examples (create encoder / decoder pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../../data/language-modeling-benchmark-r13/'\n",
    "PATH_TRAINING_CORPUS = os.path.join(PATH_DATA, 'training-monolingual.tokenized.shuffled')\n",
    "PATH_HOLDOUT_CORPUS = os.path.join(PATH_DATA, 'heldout-monolingual.tokenized.shuffled')\n",
    "\n",
    "CONTEXT_SIZE = 5\n",
    "CORPUS_FILE_COUNT = 10                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punctuation_token(token):\n",
    "    \"\"\"\n",
    "    A token is a punctuation token if the characters consist of\n",
    "    only punctuation characters\n",
    "    \"\"\"\n",
    "    return len(token) == len([c for c in token if c in punc_list])\n",
    "\n",
    "\n",
    "def get_sample_filenames():\n",
    "    \"\"\"\n",
    "    Get the names of the files to be sampled from.\n",
    "    \"\"\"\n",
    "    return os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "\n",
    "def get_raw_samples(filepath):\n",
    "    \"\"\"\n",
    "    Fetch all raw samples from a particular file path. This will pull\n",
    "    the data from the files and tokenize the samples. No pre-processing\n",
    "    is done here.\n",
    "    \n",
    "    filepath - The filepath\n",
    "    \"\"\"\n",
    "    raw_samples = []\n",
    "\n",
    "    with open(filename) as file:\n",
    "        data = file.read().split(\"\\n\")\n",
    "\n",
    "        for sentence in data:\n",
    "            raw_samples.append([t for t in word_tokenize(s.lower())])\n",
    "\n",
    "    return raw_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAMES = os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for filename in FILE_NAMES:\n",
    "    with open(filename) as file:\n",
    "        \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Network\n",
    "\n",
    "1. Create a pytorch model for processing the data.\n",
    "2. Define the negative sampling criterion.\n",
    "3. Create a training procedure for running samples through network\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating the Embeddings\n",
    "\n",
    "1. Load the sets of analogies from the datasets\n",
    "2. Find matching tokens for the analogies and discard analogies without analogous tokens\n",
    "2. Evaluate model against analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
