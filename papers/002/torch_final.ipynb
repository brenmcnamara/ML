{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "Note that the purpose of this notebook is to put together a final implementation of all the data-fetching, pre-processing, training, and evaluation of the work in the paper. There will not be much discussion on the decisions made for hyper-parameters, configurations, and other implementation details. Please look at *torch_exmplore* for an exploration on some of these implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Fetching and Pre-Processing\n",
    "\n",
    "1. Convert samples into tokens\n",
    "2. Remove punctuation-only tokens\n",
    "3. Sub-sample tokens in the vocabulary\n",
    "4. Marginalize all tokens ocurring less than 5 times in the corpus\n",
    "5. Learn phrases in the vocabulary\n",
    "6. Create training examples for negative sampling (input word + set of skip-gram output words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../../data/language-modeling-benchmark-r13/'\n",
    "PATH_TRAINING_CORPUS = os.path.join(PATH_DATA, 'training-monolingual.tokenized.shuffled')\n",
    "PATH_HOLDOUT_CORPUS = os.path.join(PATH_DATA, 'heldout-monolingual.tokenized.shuffled')\n",
    "\n",
    "CONTEXT_SIZE = 5\n",
    "CORPUS_FILE_COUNT = 10\n",
    "\n",
    "LOW_COUNT_TOKEN = '__LOW_COUNT_TOKEN__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for reading sample files from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_filenames():\n",
    "    \"\"\"\n",
    "    Get the names of the files to be sampled from.\n",
    "    \"\"\"\n",
    "    return os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "\n",
    "def get_raw_samples(filepaths):\n",
    "    \"\"\"\n",
    "    Fetch all raw samples from a particular list of file paths. This\n",
    "    will pull the data from the files and tokenize the samples. No\n",
    "    pre-processing is done here.\n",
    "    \n",
    "    filepath - A list of filepaths to fetch the raw samples.\n",
    "    \"\"\"\n",
    "    raw_samples = []\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as file:\n",
    "            data = file.read().split(\"\\n\")\n",
    "\n",
    "            for sentence in data:\n",
    "                raw_samples.append([t for t in word_tokenize(sentence.lower())])\n",
    "\n",
    "    return raw_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for calculating unigram and bigram counts of corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Given a set of samples, generate a set of unigram counts\n",
    "    within those samples.\n",
    "    \"\"\"\n",
    "    uc = {}\n",
    "    for sample in samples:\n",
    "        for token in sample:\n",
    "            if token not in uc:\n",
    "                uc[token] = 0\n",
    "            uc[token] = uc[token] + 1\n",
    "    \n",
    "    return uc\n",
    "   \n",
    "\n",
    "def create_unigram_and_bigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Given a set of samples, generate a set of unigram and\n",
    "    bigram counts within those samples.\n",
    "    \"\"\"\n",
    "    uc = {}\n",
    "    bc = {}\n",
    "    for sample in samples:\n",
    "        for i in range(len(sample)):\n",
    "            token = sample[i]\n",
    "            if token not in uc:\n",
    "                uc[token] = 0\n",
    "            uc[token] = uc[token] + 1\n",
    "            \n",
    "        for i in range(0, len(sample), 2):\n",
    "            if i + 1 >= len(sample):\n",
    "                break\n",
    "            \n",
    "            bigram = (sample[i], sample[i+1])\n",
    "            if bigram not in bc:\n",
    "                bc[bigram] = 0\n",
    "            bc[bigram] = bc[bigram] + 1\n",
    "        \n",
    "    return (uc, bc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for sub-sampling tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punctuation_token(token):\n",
    "    \"\"\"\n",
    "    A token is a punctuation token if the characters consist of\n",
    "    only punctuation characters\n",
    "    \n",
    "    token - The token to inspect.\n",
    "    \"\"\"\n",
    "    return len(token) == len([c for c in token if c in punc_list])\n",
    "\n",
    "\n",
    "def remove_pred(samples, pred):\n",
    "    \"\"\"\n",
    "    Given a predicate, remove a particular token from the\n",
    "    sample being processed. If there is at most 1 token in\n",
    "    the sample after removing the tokens, then the entire sample\n",
    "    is also removed.\n",
    "    \n",
    "    samples - The samples to remove from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    pred - A predicate that returns True if the token should be\n",
    "           kept, False otherwise.\n",
    "    \"\"\"\n",
    "    new_samples = []\n",
    "    for sample in samples:\n",
    "        new_sample = []\n",
    "\n",
    "        for token in sample:\n",
    "            if pred(token):\n",
    "                new_sample.append(token)\n",
    "                \n",
    "        if len(new_sample) > 1:\n",
    "            new_samples.append(new_sample)\n",
    "            \n",
    "    return new_samples\n",
    "    \n",
    "\n",
    "def remove_tokens(samples, tokens):\n",
    "    \"\"\"\n",
    "    Given a set of tokens, remove those tokens from the list of samples.\n",
    "    If a token exists in the set of tokens, it is removed from the set\n",
    "    of samples.\n",
    "    \n",
    "    samples - The samples to remove from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    tokens - Tokens to remove from all samples.\n",
    "    \"\"\"\n",
    "    return remove_pred(samples, lambda t: t not in tokens)\n",
    "\n",
    "\n",
    "def perform_subsampling(samples, uc, t=10e-5):\n",
    "    \"\"\"\n",
    "    Perform sub-sampling according to the procedure outlined in the paper.\n",
    "    \n",
    "    samples - The samples to sub-sample from. No changes will be\n",
    "              made to this set of samples.\n",
    "              \n",
    "    uc - The unigram counts of words.\n",
    "    \n",
    "    t - This is the t-value defined and used in the paper. Note that 10e-5\n",
    "        is chosen as the value from the paper and was calibrated to the\n",
    "        size of the corpus. This parameter is NOT independent of corpus size.\n",
    "    \"\"\"\n",
    "    tokens = uc.keys()\n",
    "    counts = np.array([uc[t] for t in tokens])\n",
    "    total_count = np.sum(counts)\n",
    "    frequencies = counts / total_count\n",
    "    \n",
    "    # p values indicate the likelihood that a particular token will\n",
    "    # be discarded. The decision to discard a token is evaluated on\n",
    "    # every instance of that token in the dataset.\n",
    "    p_values = np.maximum(0, 1 - np.sqrt(t / frequencies))\n",
    "    token_p_map = { t:p for (t, p) in zip(tokens, p_values)}\n",
    "\n",
    "    # Enumerate every token in the sample and remove it using the p values as\n",
    "    # likelihood of discarding.\n",
    "    return remove_pred(samples, lambda t: np.random.random() > token_p_map[t])\n",
    "\n",
    "\n",
    "\n",
    "def perform_low_count_marginalization(samples, uc, min_count=5):\n",
    "    \"\"\"\n",
    "    Marginalize any tokens that occur less than min_count times into a\n",
    "    special low count token.\n",
    "    \n",
    "    samples - The samples to marginalize over.\n",
    "    \n",
    "    uc - The unigram counts of tokens represented from the sample\n",
    "    \n",
    "    min_count - The lowest count of tokens allowed to remain in the dataset.\n",
    "    \"\"\"\n",
    "    new_samples = []\n",
    "\n",
    "    for sample in samples:\n",
    "        new_sample = []\n",
    "\n",
    "        for token in sample:\n",
    "            if uc[token] < min_count:\n",
    "                new_sample.append(LOW_COUNT_TOKEN)\n",
    "            else:\n",
    "                new_sample.append(token)\n",
    "                \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    return new_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for combining common phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_score(t1, t2, unigram_count, bigram_count, delta):\n",
    "    \"\"\"\n",
    "    Calculate the score for phrase combining.\n",
    "    \n",
    "    unigram_count - A dictionary mapping tokens in our vocabulary\n",
    "                    to their counts.\n",
    "                    \n",
    "    bigram_count - A dictionary mapping pairs of tokens in our vocab\n",
    "                   to their counts.\n",
    "                   \n",
    "    delta - a coefficient used to adjust the score. Higher delta means we\n",
    "            discount infrequent words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Double check that we are not dividing by 0. This should never\n",
    "    # happen in theory because if a token has 0 unigram count, it\n",
    "    # should not be in our vocab.\n",
    "    if t1 not in unigram_count or t2 not in unigram_count:\n",
    "        return 0\n",
    "    \n",
    "    t1u = unigram_count[t1]\n",
    "    t2u = unigram_count[t2]\n",
    "\n",
    "    if t1u == 0 or t2u == 0:\n",
    "        return 0\n",
    "    \n",
    "    b = bigram_count[(t1, t2)]\n",
    "    \n",
    "    return (b - delta) / (t1u * t2u)\n",
    "\n",
    "\n",
    "def create_phrase_score_map(unigram_count, bigram_count, delta):\n",
    "    \"\"\"\n",
    "    Takes a list of samples and computes a mapping of bigram phrase scoring.\n",
    "    \n",
    "    samples - A list of list of tokens.\n",
    "    \"\"\"    \n",
    "    return { (t1, t2): phrase_score(t1, t2, unigram_count, bigram_count, delta) for (t1, t2) in bigram_count.keys() }\n",
    "\n",
    "\n",
    "def merge_bigrams(samples, bigrams):\n",
    "    \"\"\"\n",
    "    Given a list of tokenized samples, create a new list of samples where bigrams\n",
    "    have been merged.\n",
    "    \n",
    "    samples - A list of samples, each sample being a list of tokens.\n",
    "    bigrams - A set of bigrams to merge.\n",
    "    \"\"\"\n",
    "    new_samples = []\n",
    "\n",
    "    for sample in samples:\n",
    "        \n",
    "        if len(sample) == 0:\n",
    "            print(\"WARNING SAMPLE LEN IS 0\", not sample)\n",
    "\n",
    "        new_sample = []\n",
    "\n",
    "        # Keep track if we merge in the previous iteration so we don't\n",
    "        # merge overlapping phrases: for (a, b, c), if (a, b) was merged\n",
    "        # we do not want to merge (b, c).\n",
    "        merged_during_previous_iter = False\n",
    "\n",
    "        for i in range(len(sample) - 1):\n",
    "            if merged_during_previous_iter:\n",
    "                merged_during_previous_iter = False\n",
    "                continue\n",
    "            \n",
    "            current = (sample[i], sample[i+1])\n",
    "            if current in bigrams:\n",
    "                new_sample.append(sample[i] + \" \" + sample[i + 1])\n",
    "                merged_during_previous_iter = True\n",
    "            else:\n",
    "                new_sample.append(sample[i])\n",
    "                \n",
    "        # We do not iterate the last element. So if the last pair was not\n",
    "        # merged, we need to add back the last token.\n",
    "        if not merged_during_previous_iter:\n",
    "            new_sample.append(sample[-1])\n",
    "\n",
    "        new_samples.append(new_sample)\n",
    "                \n",
    "    return new_samples\n",
    "\n",
    "\n",
    "def perform_combine_common_phrases(samples, uc, bc, bigram_percentile=0.95, score_percentile=0.999, passes=4):\n",
    "    \"\"\"\n",
    "    Given a set of samples and their unigram + bigram counts, we choose to combine phrases\n",
    "    found in our corpus. bigram_percentile and score_percentile are parameters used to\n",
    "    configure the phrase combination algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(passes):\n",
    "        # Figure out a good delta value using bigram quantile\n",
    "        bc_series = pd.Series(data=list(bc.values()))\n",
    "        delta = bc_series.quantile(bigram_percentile)\n",
    "    \n",
    "        # Calculate score map and threshold\n",
    "        score_map = create_phrase_score_map(uc, bc, delta)\n",
    "        score_series = pd.Series(data=list(score_map.values()))\n",
    "        score_threshold = score_series.quantile(score_percentile).item()\n",
    "    \n",
    "        # Find the phrases that have a high-enough score and generate\n",
    "        # a new set of samples with those phrases merged into a single\n",
    "        # token.\n",
    "        phrases = {b for b, s in score_map.items() if s > score_threshold}\n",
    "        samples = merge_bigrams(samples, phrases)\n",
    "        \n",
    "    return samples, phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for encoding / decoding the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(uc):\n",
    "    \"\"\"\n",
    "    Given the unigram counts in the database, define an encoding\n",
    "    and decoding map for our data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encoder is map from token to index.\n",
    "    encoder = { t:i for (i,t) in enumerate(uc.keys()) }\n",
    "    \n",
    "    # Decoder is map from index to token.\n",
    "    decoder = { i:t for (t,i) in encoder.items() }\n",
    "    \n",
    "    return (encoder, decoder)\n",
    "\n",
    "\n",
    "def encode_samples(samples, encoder):\n",
    "    encoded = []\n",
    "    for sample in samples:\n",
    "        encoded.append([encoder[t] for t in sample])\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_samples(encoded, decoder):\n",
    "    samples = []\n",
    "    for e in encoded:\n",
    "        samples.append([decoder[i] for i in e])\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for creating the training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_training_examples(samples, context=2):\n",
    "    \"\"\"\n",
    "    Generates a set of training examples for the skip-gram model with a given\n",
    "    context size.\n",
    "    \n",
    "    samples - The samples to generate training examples.\n",
    "\n",
    "    context - The size of the skip-gram learning context.\n",
    "    \"\"\"\n",
    "    window_size = context * 2 + 1\n",
    "    window_center = context\n",
    "\n",
    "    training_examples = []\n",
    "\n",
    "    for sample in samples:\n",
    "        if len(sample) < window_size:\n",
    "            # There are plenty of samples, so skipping some should be fine.\n",
    "            # Also, it would be awkward to train on non-uniform training sizes,\n",
    "            # though there may be better ways to do this.\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(sample) - window_size + 1):\n",
    "            wi = sample[i + window_center]\n",
    "            wo = sample[i:i+window_center] + sample[i+window_center+1:i+window_size]\n",
    "            training_examples.append((wi, wo))\n",
    "    \n",
    "    return training_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HERE I AM!! NEED TO TEST TRAINING EXAMPLES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = [time.time()]\n",
    "\n",
    "# Step 1: Convert data into tokenized samples.\n",
    "filepaths = get_sample_filenames()\n",
    "samples = get_raw_samples(filepaths)\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Loading data from files took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "\n",
    "# Step 2: Removing punctuation only tokens.\n",
    "samples = remove_pred(samples, lambda t: not is_punctuation_token(t))\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Removing punctuation tokens took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "\n",
    "# Step 3. Sub-sample tokens in the vocabulary.\n",
    "uc = create_unigram_counts(samples)\n",
    "samples = perform_subsampling(samples, uc)\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Sub-sampling took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "\n",
    "# Step 4. Marginalize all tokens ocurring less than 5 times\n",
    "#         in the corpus\n",
    "uc = create_unigram_counts(samples)\n",
    "samples = perform_low_count_marginalization(samples, uc)\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Marginalizing low-count tokens took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "\n",
    "# Step 5. Learn phrases in the vocabulary\n",
    "uc, bc = create_unigram_and_bigram_counts(samples)\n",
    "samples, _ = perform_combine_common_phrases(samples, uc, b)\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Learning common phrases took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "\n",
    "# Step 6: Encode the tokens in the vocabulary.\n",
    "encoder, decoder = create_encoder_and_decoder(uc)\n",
    "uc_encoded = { encoder(t):c for t,c in encoder.items() }\n",
    "\n",
    "time_points.append(time.time())\n",
    "print(f\"Encoding vocabulary took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "# Step 7. Create training examples for skip-gram model (input word + words in context).\n",
    "create_skipgram_training_examples(samples)\n",
    "time_points.append(time.time())\n",
    "print(f\"Creating negative training samples took {(time_points[-1] - time_points[-2]) / 60:.02f}m\")\n",
    "\n",
    "print(f\"Pre-processing took {(time_points[-1] - time_points[0]) / 60:.02f}m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-90-572577a258a8>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-572577a258a8>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    total_time = time.time() - start_time\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "FILE_NAMES = os.path.join(PATH_TRAINING_CORPUS, os.listdir(PATH_TRAINING_CORPUS)[:CORPUS_FILE_COUNT])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for filename in FILE_NAMES:\n",
    "    with open(filename) as file:\n",
    "        \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Network\n",
    "\n",
    "1. Create a pytorch model for processing the data.\n",
    "2. Define the negative sampling criterion.\n",
    "3. Create a training procedure for running samples through network\n",
    "4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for encoding / decoding the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_and_decoder(uc):\n",
    "    \"\"\"\n",
    "    Given the unigram counts in the database, define an encoding\n",
    "    and decoding map for our data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encoder is map from token to index.\n",
    "    encoder = { t:i for (i,t) in uc.items() }\n",
    "    \n",
    "    # Decoder is map from index to token.\n",
    "    decoder = { i:t for (t,i) in encoder.items() }\n",
    "    \n",
    "    return (encoder, decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating the Embeddings\n",
    "\n",
    "1. Load the sets of analogies from the datasets\n",
    "2. Find matching tokens for the analogies and discard analogies without analogous tokens\n",
    "2. Evaluate model against analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
