{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The corpus being used for training the neural network is not the same corpus mentioned in the paper (the Google News corpus described in the paper is not publicly available). Instead, there is an alternative corpus being used, which may affect the quality of the word vectors.\n",
    "\n",
    "## Analogies Datasets\n",
    "\n",
    "The paper mentions two datasets of analogies they used to evaluate the word embeddings generated by their network. Both of those analogies datasets are publicly available and available in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../data/language-modeling-benchmark-r13/'\n",
    "path_training_corpus = os.path.join(path_data, 'training-monolingual.tokenized.shuffled')\n",
    "path_holdout_corpus = os.path.join(path_data, 'heldout-monolingual.tokenized.shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring a single file in our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load a single file: 0.47s\n",
      "Total samples in the corpus: 306075\n"
     ]
    }
   ],
   "source": [
    "path_filename = os.path.join(path_training_corpus, os.listdir(path_training_corpus)[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(path_filename) as file:\n",
    "    samples_raw = file.read().split(\"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the words present in the file:\n",
    "- How many total words are in the file?\n",
    "- How many unique words?\n",
    "- What are the counts for each word?\n",
    "- What are the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Corpus\n",
    "\n",
    "We will use *nltk* to tokenize our corpus. As an additional pre-processing step, we will also force all our text to be lowercase. We will also remove any tokens that contains only punctuation characters. These steps may not be necessary with such a large corpus, but for this example, I do it to reduce our vocabulary size and our overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punctuation_token(token):\n",
    "    pass # TODO HERE I AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 59.51s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "samples = [word_tokenize(s.lower()) for s in samples_raw]\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 2.87s\n",
      "Total unique tokens found: 163385\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "token_freq = {}\n",
    "\n",
    "for s in samples:\n",
    "    for t in s:\n",
    "\n",
    "        if t not in token_freq:\n",
    "            token_freq[t] = 1\n",
    "        token_freq[t] = token_freq[t] + 1\n",
    "        \n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")\n",
    "\n",
    "print(f\"Total unique tokens found: {len(token_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(freq, k):\n",
    "    result = []\n",
    "\n",
    "    # Keep track of the index and count of the least frequent token in the\n",
    "    # list. This is the token that is the next candidate to be removed if\n",
    "    # we find a more frequent token.\n",
    "    token_to_remove_index = None\n",
    "    token_to_remove_count = None\n",
    "\n",
    "    # We will allow the user to enter negative numbers, indicating they\n",
    "    # want the least frequent words instead of the most frequent words.\n",
    "    should_append = lambda c: token_to_remove_count < c if k > 0 else token_to_remove_count > c\n",
    "\n",
    "    k_positive = k if k > 0 else -k\n",
    "\n",
    "    for (j, (token, count)) in enumerate(freq.items()):\n",
    "        # If we do not yet have k most frequent tokens, then we can just\n",
    "        # add this token to the list.\n",
    "        if len(result) < k_positive:\n",
    "            result.append(token)\n",
    "            # \"not should_append(c)\" indicates that the current token should\n",
    "            # be the next thing to remove when a better word comes up.\n",
    "            if token_to_remove_count is None or not should_append(count):\n",
    "                token_to_remove_count = count\n",
    "                token_to_remove_index = len(result) - 1\n",
    "            continue\n",
    "\n",
    "        # Check if this word is occurring more frequently than the least\n",
    "        # frequent token in the list.\n",
    "        if should_append(count):\n",
    "            result[token_to_remove_index] = token\n",
    "            \n",
    "            # Need to search for the next token in the list to remove.\n",
    "            token_to_remove_count = freq[result[0]]\n",
    "            token_to_remove_index = 0\n",
    "            for (i, token) in enumerate(result):\n",
    "                # \"not should_append(c)\" indicates that the current token should\n",
    "                # be the next thing to remove when a better word comes up.\n",
    "                if not should_append(freq[token]):\n",
    "                    token_to_remove_count = freq[token]\n",
    "                    token_to_remove_index = i\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of the most common words and the number of times they show up in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('as', 40323),\n",
       " (\"'s\", 70118),\n",
       " ('it', 46180),\n",
       " ('was', 47608),\n",
       " ('that', 70473),\n",
       " ('.', 310971),\n",
       " ('said', 43593),\n",
       " ('to', 184922),\n",
       " ('for', 67866),\n",
       " ('the', 416623),\n",
       " ('is', 57583),\n",
       " ('he', 39686),\n",
       " ('``', 90259),\n",
       " ('with', 46933),\n",
       " ('and', 163409),\n",
       " (',', 354051),\n",
       " ('a', 166196),\n",
       " ('of', 176199),\n",
       " ('in', 150693),\n",
       " ('on', 59740)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list mostly contains pronouns, articles, and prepositions, along with some punctuation.\n",
    "\n",
    "And also a list of the least frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demsky', 2),\n",
       " ('cocoon-like', 2),\n",
       " ('5,214', 2),\n",
       " ('permasteelisa', 2),\n",
       " ('fratelli', 2),\n",
       " ('sraya', 2),\n",
       " ('volvos', 2),\n",
       " ('seidel', 2),\n",
       " ('63,291', 2),\n",
       " ('computer-aided', 2),\n",
       " ('germanys', 2),\n",
       " ('d10', 2),\n",
       " ('amitav', 2),\n",
       " ('3-43', 2),\n",
       " ('moreh', 2),\n",
       " ('2-38', 2),\n",
       " ('s10', 2),\n",
       " ('spinboldak', 2),\n",
       " ('checkpoint-friendly', 2),\n",
       " ('yamal-europe', 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, -20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list includes long numbers (63,291), names of people (Moreh), and joined words (computer-aided, checkpoint-friendly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look for any words that contains non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_tokens = []\n",
    "\n",
    "for token in token_freq.keys():\n",
    "    found_non_alpha = False\n",
    "\n",
    "    for c in token:\n",
    "\n",
    "        if ord(c) < ord('a') or ord(c) > ord('z'):\n",
    "            found_non_alpha = True\n",
    "            continue\n",
    "        \n",
    "    if found_non_alpha:\n",
    "        non_alpha_tokens.append(token)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69% of our tokens have characters that are non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "ratio = len(non_alpha_tokens) / float(len(token_freq))\n",
    "print(f\"{ratio*100:0.2f}% of our tokens have characters that are non-alphabetical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the most common tokens with non-alpha numeric characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 310971),\n",
       " (',', 354051),\n",
       " ('?', 8331),\n",
       " (';', 5989),\n",
       " ('%', 3194),\n",
       " ('-', 10053),\n",
       " (\"'t\", 11930),\n",
       " (\"'s\", 70118),\n",
       " ('$', 12917),\n",
       " ('u.s.', 6517),\n",
       " ('``', 90259),\n",
       " (':', 13597),\n",
       " ('(', 22524),\n",
       " (\"'\", 9576),\n",
       " ('mr.', 3667),\n",
       " ('/', 5138),\n",
       " ('£', 4844),\n",
       " ('--', 22635),\n",
       " (')', 22698),\n",
       " ('10', 3375)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_alpha_freq = { t:token_freq[t] for t in non_alpha_tokens }\n",
    "\n",
    "[(t, non_alpha_freq[t]) for t in k_most_frequent(non_alpha_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list contains puncuation, years (2006, 2007), word abbreviations (Mr. and U.S.), ending of contractions ('t and 've), and common numbers (2, 10). While some of these may lead to extra noise in the data, that is fine since we have such a large corpus. To get something basic working, I'll leave most of these tokens in the vocabulary. We will do a bit of cleanup on the following:\n",
    "\n",
    "- Remove any tokens that are entirely made of punctuation\n",
    "- Marginalize all tokens that occur with a frequency of <= 5 into a single token (this was mentioned as a pre-processing step in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
