{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The corpus being used for training the neural network is not the same corpus mentioned in the paper (the Google News corpus described in the paper is not publicly available). Instead, there is an alternative corpus being used, which may affect the quality of the word vectors.\n",
    "\n",
    "## Analogies Datasets\n",
    "\n",
    "The paper mentions two datasets of analogies they used to evaluate the word embeddings generated by their network. Both of those analogies datasets are publicly available and available in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../data/language-modeling-benchmark-r13/'\n",
    "path_training_corpus = os.path.join(path_data, 'training-monolingual.tokenized.shuffled')\n",
    "path_holdout_corpus = os.path.join(path_data, 'heldout-monolingual.tokenized.shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring a single file in our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load a single file: 0.52s\n",
      "Total samples in the corpus: 306075\n"
     ]
    }
   ],
   "source": [
    "path_filename = os.path.join(path_training_corpus, os.listdir(path_training_corpus)[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(path_filename) as file:\n",
    "    samples_raw = file.read().split(\"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the words present in the file:\n",
    "- How many total words are in the file?\n",
    "- How many unique words?\n",
    "- What are the counts for each word?\n",
    "- What are the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Corpus\n",
    "\n",
    "We will use *nltk* to tokenize our corpus. As an additional pre-processing step, we will also force all our text to be lowercase. We will also remove any tokens that contains only punctuation characters. These steps may not be necessary with such a large corpus, but for this example, I do it to reduce our vocabulary size and our overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punc_list = [p for p in punctuation]\n",
    "\n",
    "def is_punctuation_token(token):\n",
    "    \"\"\"\n",
    "    A token is a punctuation token if the characters consist of\n",
    "    only punctuation characters\n",
    "    \"\"\"\n",
    "    return len(token) == len([c for c in token if c in punc_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick Test of Above Functions\n",
    "\n",
    "test1 = is_punctuation_token('.') == True\n",
    "test2 = is_punctuation_token('?)()') == True\n",
    "test3 = is_punctuation_token('...1') == False\n",
    "test4 = is_punctuation_token('abc') == False\n",
    "\n",
    "(test1, test2, test3, test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 84.18s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "samples = []\n",
    "\n",
    "for s in samples_raw:\n",
    "    tokens = [t for t in word_tokenize(s.lower()) if not is_punctuation_token(t)]\n",
    "    samples.append(tokens)\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 2.37s\n",
      "Total unique tokens found: 163351\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "token_freq = {}\n",
    "\n",
    "for s in samples:\n",
    "    for t in s:\n",
    "\n",
    "        if t not in token_freq:\n",
    "            token_freq[t] = 0\n",
    "        token_freq[t] = token_freq[t] + 1\n",
    "        \n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")\n",
    "\n",
    "print(f\"Total unique tokens found: {len(token_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(freq, k):\n",
    "    result = []\n",
    "\n",
    "    # Keep track of the index and count of the least frequent token in the\n",
    "    # list. This is the token that is the next candidate to be removed if\n",
    "    # we find a more frequent token.\n",
    "    token_to_remove_index = None\n",
    "    token_to_remove_count = None\n",
    "\n",
    "    # We will allow the user to enter negative numbers, indicating they\n",
    "    # want the least frequent words instead of the most frequent words.\n",
    "    should_append = lambda c: token_to_remove_count < c if k > 0 else token_to_remove_count > c\n",
    "\n",
    "    k_positive = k if k > 0 else -k\n",
    "\n",
    "    for (j, (token, count)) in enumerate(freq.items()):\n",
    "        # If we do not yet have k most frequent tokens, then we can just\n",
    "        # add this token to the list.\n",
    "        if len(result) < k_positive:\n",
    "            result.append(token)\n",
    "            # \"not should_append(c)\" indicates that the current token should\n",
    "            # be the next thing to remove when a better word comes up.\n",
    "            if token_to_remove_count is None or not should_append(count):\n",
    "                token_to_remove_count = count\n",
    "                token_to_remove_index = len(result) - 1\n",
    "            continue\n",
    "\n",
    "        # Check if this word is occurring more frequently than the least\n",
    "        # frequent token in the list.\n",
    "        if should_append(count):\n",
    "            result[token_to_remove_index] = token\n",
    "            \n",
    "            # Need to search for the next token in the list to remove.\n",
    "            token_to_remove_count = freq[result[0]]\n",
    "            token_to_remove_index = 0\n",
    "            for (i, token) in enumerate(result):\n",
    "                # \"not should_append(c)\" indicates that the current token should\n",
    "                # be the next thing to remove when a better word comes up.\n",
    "                if not should_append(freq[token]):\n",
    "                    token_to_remove_count = freq[token]\n",
    "                    token_to_remove_index = i\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of the most common words and the number of times they show up in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 39685),\n",
       " ('was', 47607),\n",
       " ('at', 38909),\n",
       " ('as', 40322),\n",
       " ('that', 70472),\n",
       " ('said', 43592),\n",
       " ('is', 57582),\n",
       " ('for', 67865),\n",
       " ('the', 416622),\n",
       " ('from', 33096),\n",
       " ('of', 176198),\n",
       " ('and', 163408),\n",
       " ('it', 46179),\n",
       " ('on', 59739),\n",
       " ('with', 46932),\n",
       " ('a', 166195),\n",
       " ('by', 35149),\n",
       " ('in', 150692),\n",
       " ('to', 184921),\n",
       " (\"'s\", 70117)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list mostly contains pronouns, articles, and prepositions, along with some punctuation.\n",
    "\n",
    "And also a list of the least frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demsky', 1),\n",
       " ('cocoon-like', 1),\n",
       " ('5,214', 1),\n",
       " ('permasteelisa', 1),\n",
       " ('fratelli', 1),\n",
       " ('volvos', 1),\n",
       " ('seidel', 1),\n",
       " ('63,291', 1),\n",
       " ('sraya', 1),\n",
       " ('germanys', 1),\n",
       " ('computer-aided', 1),\n",
       " ('amitav', 1),\n",
       " ('3-43', 1),\n",
       " ('moreh', 1),\n",
       " ('2-38', 1),\n",
       " ('d10', 1),\n",
       " ('spinboldak', 1),\n",
       " ('checkpoint-friendly', 1),\n",
       " ('yamal-europe', 1),\n",
       " ('s10', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, -20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list includes long numbers (63,291), names of people (Moreh), and joined words (computer-aided, checkpoint-friendly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look for any words that contains non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_tokens = []\n",
    "\n",
    "for token in token_freq.keys():\n",
    "    found_non_alpha = False\n",
    "\n",
    "    for c in token:\n",
    "\n",
    "        if ord(c) < ord('a') or ord(c) > ord('z'):\n",
    "            found_non_alpha = True\n",
    "            continue\n",
    "        \n",
    "    if found_non_alpha:\n",
    "        non_alpha_tokens.append(token)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.67% of our tokens have characters that are non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "ratio = len(non_alpha_tokens) / float(len(token_freq))\n",
    "print(f\"{ratio*100:0.2f}% of our tokens have characters that are non-alphabetical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the most common tokens with non-alpha numeric characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('15', 1770),\n",
       " (\"'re\", 2576),\n",
       " ('£', 4843),\n",
       " ('2008', 2607),\n",
       " (\"'t\", 11929),\n",
       " (\"'s\", 70117),\n",
       " ('2', 2133),\n",
       " ('u.s.', 6516),\n",
       " ('mr.', 3666),\n",
       " ('10', 3374),\n",
       " ('3', 1707),\n",
       " ('1', 2703),\n",
       " ('12', 1825),\n",
       " ('2007', 1983),\n",
       " ('11', 1800),\n",
       " ('2009', 2540),\n",
       " ('30', 2232),\n",
       " ('20', 2258),\n",
       " ('5', 1672),\n",
       " (\"'ve\", 1933)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_alpha_freq = { t:token_freq[t] for t in non_alpha_tokens }\n",
    "\n",
    "[(t, non_alpha_freq[t]) for t in k_most_frequent(non_alpha_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list contains years (2006, 2007), word abbreviations (Mr. and U.S.), ending of contractions ('t and 've), and common numbers (2, 10). While some of these may lead to extra noise in the data, that is fine since we have such a large corpus. To get something basic working, I'll leave most of these tokens in the vocabulary. We will do a bit of additional processing of the text before we can start working on the neural network:\n",
    "\n",
    "- Marginalize all tokens that occur with a frequency of <= 5 into a single token\n",
    "- Find a generate common phrases in the text and treat them as their own tokens\n",
    "\n",
    "Both of these steps were mentioned in the paper. As a side note, we will be doing the group of our corpus into phrases before we try to remove low-frequency words, since we may end up grouping some of those low frequency words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Common Phrases\n",
    "\n",
    "We will use the formula mentioned in the text:\n",
    "\n",
    "$$score(w_i, w_j) = \\frac {count(w_i, w_j) - \\delta} {count(w_i) \\times count(w_j)}$$\n",
    "\n",
    "The score between any 2 tokens is computed based on their bigram and unigram counts. We've already calculated the unigram counts of every token when we collected their frequencies. We will also need to compute the bigram counts of every pair of tokens in our vocabulary.\n",
    "\n",
    "The $\\delta$ used is a discounting coefficient to prevent too many phrases consisting of infrequent words from forming.\n",
    "\n",
    "Note that $\\delta$ is a coefficient that will be dependent on the corpus size (not normalized), so a value used on some text examples may not extend well to the general training corpus.\n",
    "\n",
    "We need to also choose a score threshold that indicates 2 tokens are merging. After doing a few passes over the entire corpus, we may end up with phrases of 3+ words. Note that after each pass of the database, we will need to recompute our unigram and bigram counts since the set of tokens changes.\n",
    "\n",
    "We will also keep a mapping of which tokens are combined to process our samples after the phrases have been combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_and_bigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Calculate the unigram and bigram counts of our samples.\n",
    "    \n",
    "    Samples - A list of list of tokens. The our list is a single\n",
    "              sample. Each sample contains a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    unigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "\n",
    "    for s in samples:\n",
    "        # Unigram count calculation\n",
    "        for i in range(len(s)):\n",
    "            unigram = s[i]\n",
    "            if unigram not in unigram_counts:\n",
    "                unigram_counts[unigram] = 0\n",
    "            unigram_counts[unigram] = unigram_counts[unigram] + 1\n",
    "               \n",
    "        # Bigram count calculation\n",
    "        for i in range(0, len(s), 2):\n",
    "            # Quit if we do not have enough tokens to form\n",
    "            # more bigrams.\n",
    "            if i + 1 >= len(s):\n",
    "                break\n",
    "                \n",
    "            bigram = (s[i], s[i+1])\n",
    "\n",
    "            if bigram not in bigram_counts:\n",
    "                bigram_counts[bigram] = 0\n",
    "            bigram_counts[bigram] = bigram_counts[bigram] + 1\n",
    "            \n",
    "    return (unigram_counts, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_score(t1, t2, unigram_count, bigram_count, delta):\n",
    "    \"\"\"\n",
    "    Calculate the score for phrase combining.\n",
    "    \n",
    "    unigram_count - A dictionary mapping tokens in our vocabulary\n",
    "                    to their counts.\n",
    "                    \n",
    "    bigram_count - A dictionary mapping pairs of tokens in our vocab\n",
    "                   to their counts.\n",
    "                   \n",
    "    delta - a coefficient used to adjust the score. Higher delta means we\n",
    "            discount infrequent words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Double check that we are not dividing by 0. This should never\n",
    "    # happen in theory because if a token has 0 unigram count, it\n",
    "    # should not be in our vocab.\n",
    "    if t1 not in unigram_count or t2 not in unigram_count:\n",
    "        return 0\n",
    "    \n",
    "    t1u = unigram_count[t1]\n",
    "    t2u = unigram_count[t2]\n",
    "\n",
    "    if t1u == 0 or t2u == 0:\n",
    "        return 0\n",
    "    \n",
    "    b = bigram_count[(t1, t2)]\n",
    "    \n",
    "    return (b - delta) / (t1u * t2u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_map(samples, delta):\n",
    "    \"\"\"\n",
    "    Takes a list of samples and computes a mapping of bigram phrase scoring.\n",
    "    \n",
    "    samples - A list of list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    (unigram_count, bigram_count) = calculate_unigram_and_bigram_counts(samples)\n",
    "    \n",
    "    return { (t1, t2): phrase_score(t1, t2, unigram_count, bigram_count, delta) for (t1, t2) in bigram_count.keys() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 7.61s to run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('farouq', 'al-qadoumi'), 1.0),\n",
       " (('uag', 'tecos'), 1.0),\n",
       " (('dhia', 'al-kawaz'), 1.0),\n",
       " (('01456', '486358'), 1.0),\n",
       " (('akreos', 'sofport'), 1.0),\n",
       " (('jennet', 'mallow'), 1.0),\n",
       " (('total-immersion', 'how-to-be-a-jack0ff-idiot'), 1.0),\n",
       " (('carloforto', 'isola'), 1.0),\n",
       " (('danuta', 'budorina'), 1.0),\n",
       " (('asawat', 'al-iraq'), 1.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "score_map = create_score_map(samples, delta=0)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"This took {total_time:0.2f}s to run\")\n",
    "\n",
    "[(b, score_map[b]) for b in k_most_frequent(score_map, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that having a delta of 0 results in a lot of matching for pairs of tokens that only occur once in the text. We will change the delta value to penalize low-frequency words.\n",
    "\n",
    "Let's try a few different delta values to see which gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 20 completed\n",
      "2 / 20 completed\n",
      "3 / 20 completed\n",
      "4 / 20 completed\n",
      "5 / 20 completed\n",
      "6 / 20 completed\n",
      "7 / 20 completed\n",
      "8 / 20 completed\n",
      "9 / 20 completed\n",
      "10 / 20 completed\n",
      "11 / 20 completed\n",
      "12 / 20 completed\n",
      "13 / 20 completed\n",
      "14 / 20 completed\n",
      "15 / 20 completed\n",
      "16 / 20 completed\n",
      "17 / 20 completed\n",
      "18 / 20 completed\n",
      "19 / 20 completed\n",
      "20 / 20 completed\n",
      "This took 2.77m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "deltas = np.logspace(-1, 3, 20)\n",
    "\n",
    "freqs = []\n",
    "\n",
    "for (i, delta) in enumerate(deltas):\n",
    "    score_map = create_score_map(samples, delta)\n",
    "    freq = k_most_frequent(score_map, 40)\n",
    "    freqs.append([(b, score_map[b]) for b in freq])\n",
    "    print(f\"{i+1:02} / {len(deltas)} completed\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"This took {total_time/60:0.2f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.1\n",
      "[('nickname-less', 'whites-only'), ('01456', '486358'), ('anarchism', 'co-exists'), ('poddala', 'jayantha'), ('landrum', 'papered'), ('3888', 'ecocentric.co.uk'), ('dorks', 'jocks'), ('jennet', 'mallow'), ('vujic', 'valjevo'), ('danuta', 'budorina'), ('nicolay', 'bogachev'), ('sural', 'ncv'), ('dhia', 'al-kawaz'), ('valéry', 'lucentini'), ('carloforto', 'isola'), ('4777', 'www.globalgardens.co.uk'), ('wamidh', 'nadhmi'), ('shantanu', 'narayan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('rías', 'baixas'), ('farouq', 'al-qadoumi'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('0532', '793888'), ('162.42', '0.6963'), ('career-long', '57-yarder'), ('aramide', 'olaniyan'), ('violito', 'payla'), ('natasharichardson', 'lindsaylohan'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('rais', 'yatim'), ('mylonitic', 'graphitic'), ('galp', 'energia'), ('t.mccoy', '1-17'), ('bullfighter', 'escamillo'), ('3-36', 'ma.bennett'), ('uag', 'tecos'), ('greater-spotted', 'woodpeckers'), ('akreos', 'sofport'), ('asawat', 'al-iraq')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.16237767391887217\n",
      "[('nickname-less', 'whites-only'), ('anarchism', 'co-exists'), ('landrum', 'papered'), ('farouq', 'al-qadoumi'), ('vujic', 'valjevo'), ('danuta', 'budorina'), ('3888', 'ecocentric.co.uk'), ('poddala', 'jayantha'), ('asawat', 'al-iraq'), ('dorks', 'jocks'), ('nicolay', 'bogachev'), ('jennet', 'mallow'), ('01456', '486358'), ('natasharichardson', 'lindsaylohan'), ('valéry', 'lucentini'), ('sural', 'ncv'), ('dhia', 'al-kawaz'), ('4777', 'www.globalgardens.co.uk'), ('chelyabinsk', 'nizhny'), ('akreos', 'sofport'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('mandu', 'magandi'), ('carloforto', 'isola'), ('0532', '793888'), ('162.42', '0.6963'), ('career-long', '57-yarder'), ('violito', 'payla'), ('uag', 'tecos'), ('wamidh', 'nadhmi'), ('rías', 'baixas'), ('rais', 'yatim'), ('mylonitic', 'graphitic'), ('shantanu', 'narayan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('t.mccoy', '1-17'), ('bullfighter', 'escamillo'), ('aramide', 'olaniyan'), ('galp', 'energia'), ('3-36', 'ma.bennett'), ('greater-spotted', 'woodpeckers')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.26366508987303583\n",
      "[('vujic', 'valjevo'), ('nickname-less', 'whites-only'), ('anarchism', 'co-exists'), ('asawat', 'al-iraq'), ('jennet', 'mallow'), ('poddala', 'jayantha'), ('uag', 'tecos'), ('dorks', 'jocks'), ('nicolay', 'bogachev'), ('3888', 'ecocentric.co.uk'), ('4777', 'www.globalgardens.co.uk'), ('valéry', 'lucentini'), ('wamidh', 'nadhmi'), ('chelyabinsk', 'nizhny'), ('natasharichardson', 'lindsaylohan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('dhia', 'al-kawaz'), ('mandu', 'magandi'), ('0532', '793888'), ('01456', '486358'), ('shantanu', 'narayan'), ('162.42', '0.6963'), ('farouq', 'al-qadoumi'), ('landrum', 'papered'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('rías', 'baixas'), ('career-long', '57-yarder'), ('violito', 'payla'), ('danuta', 'budorina'), ('rais', 'yatim'), ('sural', 'ncv'), ('akreos', 'sofport'), ('mylonitic', 'graphitic'), ('t.mccoy', '1-17'), ('aramide', 'olaniyan'), ('3-36', 'ma.bennett'), ('galp', 'energia'), ('bullfighter', 'escamillo'), ('carloforto', 'isola'), ('greater-spotted', 'woodpeckers')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.42813323987193935\n",
      "[('nickname-less', 'whites-only'), ('4777', 'www.globalgardens.co.uk'), ('anarchism', 'co-exists'), ('asawat', 'al-iraq'), ('dhia', 'al-kawaz'), ('poddala', 'jayantha'), ('wamidh', 'nadhmi'), ('01456', '486358'), ('natasharichardson', 'lindsaylohan'), ('dorks', 'jocks'), ('sural', 'ncv'), ('3888', 'ecocentric.co.uk'), ('nicolay', 'bogachev'), ('rías', 'baixas'), ('valéry', 'lucentini'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('landrum', 'papered'), ('farouq', 'al-qadoumi'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('0532', '793888'), ('162.42', '0.6963'), ('vujic', 'valjevo'), ('career-long', '57-yarder'), ('bullfighter', 'escamillo'), ('violito', 'payla'), ('uag', 'tecos'), ('rais', 'yatim'), ('danuta', 'budorina'), ('mylonitic', 'graphitic'), ('jennet', 'mallow'), ('shantanu', 'narayan'), ('aramide', 'olaniyan'), ('t.mccoy', '1-17'), ('3-36', 'ma.bennett'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('greater-spotted', 'woodpeckers'), ('akreos', 'sofport'), ('galp', 'energia'), ('carloforto', 'isola')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.6951927961775606\n",
      "[('dorks', 'jocks'), ('nicolay', 'bogachev'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('4777', 'www.globalgardens.co.uk'), ('dhia', 'al-kawaz'), ('valéry', 'lucentini'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('wamidh', 'nadhmi'), ('uag', 'tecos'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('nlb', 'skladi'), ('cheon', 'ho-seon'), ('dhondup', 'wangchen'), ('www.videonewswire.com', 'event.asp'), ('anatoliy', 'tymoschuk'), ('idris', 'elba'), ('bacillus', 'thuringiensis'), ('chichén', 'itzá'), ('tarsis', 'kabwegyere'), ('jakup', 'krasniqi'), ('plasmodium', 'falciparum'), ('jakrapob', 'penkair'), ('dhi', 'qar'), ('martiga', 'lohn'), ('diosdado', 'cabello'), ('636-4100', 'bam.org'), ('kimmo', 'timonen'), ('aravind', 'adiga'), ('jair', 'jurrjens'), ('glioblastoma', 'multiforme'), ('asharq', 'al-awsat'), ('anousha', 'sakoui'), ('shaila', 'dewan'), ('tarique', 'ghaffur'), ('al-haram', 'al-sharif'), ('jerel', 'mcneal'), ('ritt', 'bjerregaard'), ('paavo', 'lipponen'), ('jamia', 'millia')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1.1288378916846888\n",
      "[('jamia', 'millia'), ('glioblastoma', 'multiforme'), ('abed', 'rabbo'), ('keyon', 'dooling'), ('diosdado', 'cabello'), ('dhi', 'qar'), ('nlb', 'skladi'), ('al-haram', 'al-sharif'), ('sandip', 'moneea'), ('tarique', 'ghaffur'), ('oyu', 'tolgoi'), ('ondrej', 'pavelec'), ('cheon', 'ho-seon'), ('gazzetta', 'dello'), ('ulan', 'bator'), ('dhondup', 'wangchen'), ('asharq', 'al-awsat'), ('shaila', 'dewan'), ('kimmo', 'timonen'), ('paavo', 'lipponen'), ('anousha', 'sakoui'), ('www.videonewswire.com', 'event.asp'), ('anatoliy', 'tymoschuk'), ('idris', 'elba'), ('aravind', 'adiga'), ('yad', 'vashem'), ('martiga', 'lohn'), ('bacillus', 'thuringiensis'), ('chichén', 'itzá'), ('636-4100', 'bam.org'), ('tarsis', 'kabwegyere'), ('ritt', 'bjerregaard'), ('jakup', 'krasniqi'), ('jerel', 'mcneal'), ('shami', 'chakrabarti'), ('plasmodium', 'falciparum'), ('jakrapob', 'penkair'), ('fait', 'accompli'), ('niclas', 'fasth'), ('jair', 'jurrjens')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1.8329807108324356\n",
      "[('nursultan', 'nazarbayev'), ('vitaly', 'churkin'), ('emmerson', 'mnangagwa'), ('palos', 'verdes'), ('austan', 'goolsbee'), ('kelenna', 'azubuike'), ('fareed', 'zakaria'), ('esa-pekka', 'salonen'), ('younes', 'kaboul'), ('sandip', 'moneea'), ('barisan', 'nasional'), ('zalmay', 'khalilzad'), ('methicillin-resistant', 'staphylococcus'), ('kenji', 'johjima'), ('clermont', 'auvergne'), ('salva', 'kiir'), ('zahi', 'hawass'), ('ulan', 'bator'), ('fait', 'accompli'), ('zooey', 'deschanel'), ('farhatullah', 'babar'), ('sheikha', 'lubna'), ('bronislaw', 'komorowski'), ('aitzaz', 'ahsan'), ('warburg', 'pincus'), ('hiroki', 'kuroda'), ('ruslan', 'fedotenko'), ('ku', 'klux'), ('ondrej', 'pavelec'), ('edin', 'dzeko'), ('jesper', 'parnevik'), ('chone', 'figgins'), ('jaroslav', 'halak'), ('hallowe', \"'en\"), ('oyu', 'tolgoi'), ('anquan', 'boldin'), ('yad', 'vashem'), ('shami', 'chakrabarti'), ('gazzetta', 'dello'), ('niclas', 'fasth')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 2.9763514416313175\n",
      "[('anquan', 'boldin'), ('sal', 'oppenheim'), ('hector', 'sants'), ('corpus', 'christi'), ('antawn', 'jamison'), ('ruslan', 'fedotenko'), ('anders', 'fogh'), ('heikki', 'kovalainen'), ('frank-walter', 'steinmeier'), ('maulana', 'fazlullah'), ('zalmay', 'khalilzad'), ('neelie', 'kroes'), ('gwyneth', 'paltrow'), ('methicillin-resistant', 'staphylococcus'), ('atrial', 'fibrillation'), ('ilya', 'kovalchuk'), ('marat', 'safin'), ('younes', 'kaboul'), ('susilo', 'bambang'), ('hallowe', \"'en\"), ('preah', 'vihear'), ('ku', 'klux'), ('austan', 'goolsbee'), ('hiroki', 'kuroda'), ('virender', 'sehwag'), ('phnom', 'penh'), ('amare', 'stoudemire'), ('ulan', 'bator'), ('tal', 'afar'), ('chone', 'figgins'), ('flavio', 'briatore'), ('izabella', 'kaminska'), ('dunkin', 'donuts'), ('crédit', 'agricole'), ('hedo', 'turkoglu'), ('warburg', 'pincus'), ('burkina', 'faso'), ('société', 'générale'), ('barisan', 'nasional'), ('sumitomo', 'mitsui')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 4.832930238571752\n",
      "[('yar', \"'adua\"), ('corpus', 'christi'), ('ku', 'klux'), ('luiz', 'inacio'), ('amare', 'stoudemire'), ('slobodan', 'milosevic'), ('dera', 'ismail'), ('kung', 'fu'), ('timo', 'glock'), ('tal', 'afar'), ('uttar', 'pradesh'), ('izabella', 'kaminska'), ('neelie', 'kroes'), ('younes', 'kaboul'), ('preah', 'vihear'), ('susilo', 'bambang'), ('kaiser', 'permanente'), ('lamar', 'odom'), ('northrop', 'grumman'), ('société', 'générale'), ('pau', 'gasol'), ('khmer', 'rouge'), ('heikki', 'kovalainen'), ('flavio', 'briatore'), ('recep', 'tayyip'), ('buenos', 'aires'), ('gwyneth', 'paltrow'), ('sergey', 'brin'), ('otc', 'bulletin'), ('burkina', 'faso'), ('mamma', 'mia'), ('wes', 'streeting'), ('kuala', 'lumpur'), ('frank-walter', 'steinmeier'), ('hedo', 'turkoglu'), ('phnom', 'penh'), ('terra', 'firma'), ('lashkar', 'gah'), ('meryl', 'streep'), ('dunkin', 'donuts')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 7.847599703514611\n",
      "[('sao', 'paulo'), ('northrop', 'grumman'), ('costa', 'rica'), ('kaiser', 'permanente'), ('quentin', 'tarantino'), ('tel', 'aviv'), ('hedo', 'turkoglu'), ('shi', \"'ite\"), ('pau', 'gasol'), ('kuala', 'lumpur'), ('wen', 'jiabao'), ('luiz', 'inacio'), ('padraig', 'harrington'), ('sergey', 'brin'), ('recep', 'tayyip'), ('dunkin', 'donuts'), ('roland', 'garros'), ('flavio', 'briatore'), ('meryl', 'streep'), ('lashkar', 'gah'), ('sinn', 'fein'), ('gon', 'na'), ('thaksin', 'shinawatra'), ('lamar', 'odom'), ('exxon', 'mobil'), ('umar', 'farouk'), ('notre', 'dame'), ('tayyip', 'erdogan'), ('cerebral', 'palsy'), ('slobodan', 'milosevic'), ('gwyneth', 'paltrow'), ('izabella', 'kaminska'), ('buenos', 'aires'), ('suu', 'kyi'), ('otc', 'bulletin'), ('angelina', 'jolie'), ('kung', 'fu'), ('khmer', 'rouge'), ('terra', 'firma'), ('dirk', 'nowitzki')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 12.742749857031335\n",
      "[('gon', 'na'), ('sri', 'lanka'), ('arnold', 'schwarzenegger'), ('â', '®'), ('abu', 'dhabi'), ('dmitry', 'medvedev'), ('benedict', 'xvi'), ('goldman', 'sachs'), ('benazir', 'bhutto'), ('britney', 'spears'), ('roland', 'garros'), ('las', 'vegas'), ('fannie', 'mae'), ('angela', 'merkel'), ('notre', 'dame'), ('l', \"'aquila\"), ('buenos', 'aires'), ('padraig', 'harrington'), ('fabio', 'capello'), ('exxon', 'mobil'), ('differ', 'materially'), ('lib', 'dems'), ('otc', 'bulletin'), ('sinn', 'fein'), ('alistair', 'darling'), ('shi', \"'ite\"), ('cristiano', 'ronaldo'), ('des', 'moines'), ('tel', 'aviv'), ('hu', 'jintao'), ('osama', 'bin'), ('dalai', 'lama'), ('thaksin', 'shinawatra'), ('khmer', 'rouge'), ('suu', 'kyi'), ('kuala', 'lumpur'), ('puerto', 'rico'), ('hamid', 'karzai'), ('costa', 'rica'), ('‚', 'â')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 20.6913808111479\n",
      "[('las', 'vegas'), ('goldman', 'sachs'), ('dalai', 'lama'), ('notre', 'dame'), ('nicolas', 'sarkozy'), ('puerto', 'rico'), ('freddie', 'mac'), ('hong', 'kong'), ('walt', 'disney'), ('fannie', 'mae'), ('suu', 'kyi'), ('buenos', 'aires'), ('gon', 'na'), ('sri', 'lanka'), ('merrill', 'lynch'), ('hamid', 'karzai'), ('swine', 'flu'), ('abu', 'dhabi'), ('quote', 'profile'), ('differ', 'materially'), ('nancy', 'pelosi'), ('sri', 'lankan'), ('greenhouse', 'gases'), ('wells', 'fargo'), ('arnold', 'schwarzenegger'), ('iron', 'ore'), ('ac', 'milan'), ('benazir', 'bhutto'), ('saudi', 'arabia'), ('â', '®'), ('angela', 'merkel'), ('bin', 'laden'), ('bear', 'stearns'), ('dmitry', 'medvedev'), ('alistair', 'darling'), ('osama', 'bin'), ('mitt', 'romney'), ('tel', 'aviv'), ('saddam', 'hussein'), ('carbon', 'dioxide')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 33.59818286283781\n",
      "[('nicolas', 'sarkozy'), ('greenhouse', 'gases'), ('merrill', 'lynch'), ('nancy', 'pelosi'), ('hamid', 'karzai'), ('grand', 'slam'), ('osama', 'bin'), ('quote', 'profile'), ('sri', 'lanka'), ('abu', 'dhabi'), ('swine', 'flu'), ('super', 'bowl'), ('fannie', 'mae'), ('capitol', 'hill'), ('goldman', 'sachs'), ('alistair', 'darling'), ('bin', 'laden'), ('morgan', 'stanley'), ('tampa', 'bay'), ('dalai', 'lama'), ('forward-looking', 'statements'), ('notre', 'dame'), ('suu', 'kyi'), ('al', 'qaeda'), ('pleaded', 'guilty'), ('los', 'angeles'), ('lehman', 'brothers'), ('carbon', 'dioxide'), ('hillary', 'rodham'), ('freddie', 'mac'), ('�', '�'), ('hong', 'kong'), ('san', 'francisco'), ('san', 'diego'), ('arnold', 'schwarzenegger'), ('saudi', 'arabia'), ('st.', 'louis'), ('bear', 'stearns'), ('las', 'vegas'), ('sarah', 'palin')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 54.555947811685144\n",
      "[('quote', 'profile'), ('pleaded', 'guilty'), ('w.', 'bush'), ('las', 'vegas'), ('sarah', 'palin'), ('dalai', 'lama'), ('bin', 'laden'), ('forward-looking', 'statements'), ('human', 'rights'), ('san', 'francisco'), ('saudi', 'arabia'), ('hamid', 'karzai'), ('premier', 'league'), ('super', 'bowl'), ('abu', 'dhabi'), ('los', 'angeles'), ('san', 'diego'), ('fannie', 'mae'), ('real', 'estate'), ('wall', 'street'), ('morgan', 'stanley'), ('carbon', 'dioxide'), ('per', 'cent'), ('�', '�'), ('northern', 'ireland'), ('web', 'site'), ('st.', 'louis'), ('tampa', 'bay'), ('prime', 'minister'), ('goldman', 'sachs'), ('hillary', 'rodham'), ('global', 'warming'), ('sri', 'lanka'), ('rodham', 'clinton'), ('hong', 'kong'), ('al', 'qaeda'), ('gordon', 'brown'), ('associated', 'press'), ('swine', 'flu'), ('george', 'w.')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 88.58667904100822\n",
      "[('associated', 'press'), ('web', 'site'), ('barack', 'obama'), ('federal', 'reserve'), ('wall', 'street'), ('global', 'warming'), ('middle', 'east'), ('gordon', 'brown'), ('forward-looking', 'statements'), ('health', 'care'), ('hong', 'kong'), ('profile', 'research'), ('san', 'diego'), ('las', 'vegas'), ('climate', 'change'), ('south', 'africa'), ('northern', 'ireland'), ('�', '�'), ('click', 'here'), ('george', 'w.'), ('w.', 'bush'), ('vice', 'president'), ('north', 'korea'), ('prime', 'minister'), ('premier', 'league'), ('united', 'states'), ('supreme', 'court'), ('los', 'angeles'), ('fourth', 'quarter'), ('human', 'rights'), ('al', 'qaeda'), ('white', 'house'), ('real', 'estate'), ('chief', 'executive'), ('san', 'francisco'), ('quote', 'profile'), ('interest', 'rates'), ('per', 'cent'), ('saudi', 'arabia'), ('swine', 'flu')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 143.8449888287663\n",
      "[('wall', 'street'), ('barack', 'obama'), ('interest', 'rates'), ('years', 'ago'), ('federal', 'reserve'), ('real', 'estate'), ('didn', \"'t\"), ('north', 'korea'), ('hong', 'kong'), ('washington', 'ap'), ('las', 'vegas'), ('doesn', \"'t\"), ('per', 'cent'), ('i', 'am'), ('isn', \"'t\"), ('san', 'francisco'), ('wouldn', \"'t\"), ('chief', 'executive'), ('web', 'site'), ('human', 'rights'), ('associated', 'press'), ('health', 'care'), ('san', 'diego'), ('south', 'africa'), ('premier', 'league'), ('los', 'angeles'), ('don', \"'t\"), ('supreme', 'court'), ('i', \"'m\"), ('rather', 'than'), ('john', 'mccain'), ('wasn', \"'t\"), ('white', 'house'), ('new', 'york'), ('vice', 'president'), ('climate', 'change'), ('gordon', 'brown'), ('�', '�'), ('prime', 'minister'), ('united', 'states')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 233.57214690901213\n",
      "[('i', 'think'), ('don', \"'t\"), ('per', 'cent'), ('isn', \"'t\"), ('more', 'than'), ('last', 'week'), ('washington', 'ap'), ('i', \"'m\"), ('prime', 'minister'), ('web', 'site'), ('i', \"'ve\"), ('associated', 'press'), ('did', 'not'), ('no', 'longer'), ('last', 'month'), ('san', 'francisco'), ('at', 'least'), ('years', 'ago'), ('wall', 'street'), ('we', \"'re\"), ('wasn', \"'t\"), ('barack', 'obama'), ('last', 'year'), ('los', 'angeles'), ('new', 'york'), ('rather', 'than'), ('human', 'rights'), ('so', 'far'), ('white', 'house'), ('i', 'don'), ('gordon', 'brown'), ('health', 'care'), ('chief', 'executive'), ('united', 'states'), ('less', 'than'), ('doesn', \"'t\"), ('climate', 'change'), ('i', 'am'), ('north', 'korea'), ('didn', \"'t\")]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 379.2690190732246\n",
      "[('i', 'think'), ('last', 'year'), ('i', \"'m\"), ('as', 'well'), ('this', 'week'), ('last', 'week'), ('last', 'month'), ('i', 'don'), ('don', \"'t\"), ('white', 'house'), ('will', 'be'), ('at', 'least'), ('this', 'year'), ('health', 'care'), ('did', 'not'), ('according', 'to'), ('years', 'ago'), ('you', 'can'), ('we', \"'re\"), ('new', 'york'), ('i', \"'ve\"), ('so', 'far'), ('had', 'been'), ('united', 'states'), ('such', 'as'), ('didn', \"'t\"), ('i', 'am'), ('has', 'been'), ('rather', 'than'), ('barack', 'obama'), ('more', 'than'), ('there', 'are'), ('prime', 'minister'), ('if', 'you'), ('doesn', \"'t\"), ('would', 'be'), ('los', 'angeles'), ('per', 'cent'), ('have', 'been'), ('chief', 'executive')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 615.8482110660261\n",
      "[('i', 'think'), ('did', 'not'), ('there', 'is'), ('you', 'can'), ('according', 'to'), ('last', 'week'), ('it', \"'s\"), ('could', 'be'), ('had', 'been'), ('i', \"'m\"), ('los', 'angeles'), ('it', 'is'), ('prime', 'minister'), ('have', 'been'), ('there', 'are'), ('last', 'year'), ('we', 'are'), ('this', 'year'), ('i', 'am'), ('more', 'than'), ('this', 'week'), ('new', 'york'), ('one', 'of'), ('he', 'was'), ('will', 'be'), ('per', 'cent'), ('part', 'of'), ('don', \"'t\"), ('has', 'been'), ('they', 'are'), ('would', 'be'), ('at', 'least'), ('he', 'said'), ('as', 'well'), ('such', 'as'), ('didn', \"'t\"), ('they', 'were'), ('united', 'states'), ('it', 'was'), ('if', 'you')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1000.0\n",
      "[('it', 'is'), ('the', 'first'), ('at', 'least'), ('they', 'are'), ('had', 'been'), ('per', 'cent'), ('of', 'the'), ('will', 'be'), ('in', 'the'), ('this', 'is'), ('it', \"'s\"), ('he', 'said'), ('the', 'same'), ('would', 'be'), ('such', 'as'), ('this', 'year'), ('he', 'was'), ('i', \"'m\"), ('part', 'of'), ('the', 'united'), ('there', 'are'), ('one', 'of'), ('more', 'than'), ('don', \"'t\"), ('there', 'is'), ('united', 'states'), ('to', 'make'), ('the', 'world'), ('as', 'well'), ('to', 'be'), ('new', 'york'), ('as', 'a'), ('last', 'year'), ('the', 'company'), ('they', 'were'), ('according', 'to'), ('has', 'been'), ('at', 'the'), ('it', 'was'), ('have', 'been')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(deltas)):\n",
    "    print(f\"Delta: {deltas[i]}\")\n",
    "    print([b for b, _ in freqs[i]])\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that low delta values favor very rare words but very high delta values favor very common words. This is likely because for high delta values, the most important thing is our bigram count is larger than delta. For a delta of 1000, anything that has a bigram count of less than 1000 will automatically get scored less than bigrams with a count of greater than 1000. In this way, you can think of the delta score as thresholding bigram counts that we care about.\n",
    "\n",
    "With this in mind, let's look at some statistics of bigram counts in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bigram_counts_map = calculate_unigram_and_bigram_counts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [f + \" \" + s for f,s in bigram_counts_map.keys()]\n",
    "vals = list(bigram_counts_map.values())\n",
    "bigram_series = pd.Series(index=keys, data=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.164472e+06\n",
       "mean     2.887178e+00\n",
       "std      3.837628e+01\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      1.000000e+00\n",
       "75%      1.000000e+00\n",
       "max      2.073000e+04\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our statistics that the vast majority of bigrams frequencies of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7545840518277812"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_series[bigram_series == 1]) / len(bigram_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick search over quantiles of our series to get a better understanding of how our data is spread out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9, 3.0),\n",
       " (0.99, 28.0),\n",
       " (0.999, 179.0),\n",
       " (0.9999, 917.0),\n",
       " (0.99999, 3934.423200007528),\n",
       " (0.999999, 18025.34677637741),\n",
       " (0.9999999, 20636.14363762783),\n",
       " (0.99999999, 20720.614363668952),\n",
       " (0.999999999, 20729.06143634813),\n",
       " (0.9999999999, 20729.906143540982)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(q, bigram_series.quantile(q)) for q in 1 - np.logspace(-1, -10, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
