{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The corpus being used for training the neural network is not the same corpus mentioned in the paper (the Google News corpus described in the paper is not publicly available). Instead, there is an alternative corpus being used, which may affect the quality of the word vectors.\n",
    "\n",
    "## Analogies Datasets\n",
    "\n",
    "The paper mentions two datasets of analogies they used to evaluate the word embeddings generated by their network. Both of those analogies datasets are publicly available and available in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../data/language-modeling-benchmark-r13/'\n",
    "path_training_corpus = os.path.join(path_data, 'training-monolingual.tokenized.shuffled')\n",
    "path_holdout_corpus = os.path.join(path_data, 'heldout-monolingual.tokenized.shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring a single file in our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load a single file: 0.46s\n",
      "Total samples in the corpus: 306075\n"
     ]
    }
   ],
   "source": [
    "path_filename = os.path.join(path_training_corpus, os.listdir(path_training_corpus)[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(path_filename) as file:\n",
    "    samples_raw = file.read().split(\"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the words present in the file:\n",
    "- How many total words are in the file?\n",
    "- How many unique words?\n",
    "- What are the counts for each word?\n",
    "- What are the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 59.96s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "samples = [word_tokenize(s) for s in samples_raw]\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 2.57s\n",
      "Total unique tokens found: 186220\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "token_freq = {}\n",
    "\n",
    "for s in samples:\n",
    "    for t in s:\n",
    "\n",
    "        if t not in token_freq:\n",
    "            token_freq[t] = 1\n",
    "        token_freq[t] = token_freq[t] + 1\n",
    "        \n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")\n",
    "\n",
    "print(f\"Total unique tokens found: {len(token_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(freq, k):\n",
    "    most_freq = []\n",
    "\n",
    "    # Keep track of the index and count of the least frequent token in the\n",
    "    # list. This is the token that is the next candidate to be removed if\n",
    "    # we find a more frequent token.\n",
    "    token_to_remove_index = None\n",
    "    token_to_remove_count = None\n",
    "\n",
    "    for (token, count) in freq.items():\n",
    "        # If we do not yet have k most frequent tokens, then we can just\n",
    "        # add this token to the list.\n",
    "        if len(most_freq) < k:\n",
    "            most_freq.append(token)\n",
    "            if token_to_remove_count is None or count < token_to_remove_count:\n",
    "                token_to_remove_count = count\n",
    "                token_to_remove_index = len(most_freq) - 1\n",
    "            continue\n",
    "\n",
    "        # Check if this word is occurring more frequently than the least\n",
    "        # frequent token in the list.\n",
    "        if token_to_remove_count < count:\n",
    "            most_freq[token_to_remove_index] = token\n",
    "            \n",
    "            # Need to search for the least frequent token in the list.\n",
    "            token_to_remove_count = freq[most_freq[0]]\n",
    "            token_to_remove_index = 0\n",
    "            for (i, token) in enumerate(most_freq):\n",
    "                if freq[token] < token_to_remove_count:\n",
    "                    token_to_remove_count = freq[token]\n",
    "                    token_to_remove_index = i\n",
    "    \n",
    "    return most_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of the most common words and the number of times they show up in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45329, 'with'),\n",
       " (43543, 'said'),\n",
       " (47491, 'was'),\n",
       " (65316, 'for'),\n",
       " (67363, 'that'),\n",
       " (311503, '.'),\n",
       " (57695, 'on'),\n",
       " (57011, 'is'),\n",
       " (354051, ','),\n",
       " (53410, 'The'),\n",
       " (70058, \"'s\"),\n",
       " (183646, 'to'),\n",
       " (90259, '``'),\n",
       " (37629, 'as'),\n",
       " (159188, 'and'),\n",
       " (36604, 'at'),\n",
       " (156645, 'a'),\n",
       " (175486, 'of'),\n",
       " (141074, 'in'),\n",
       " (362986, 'the')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token_freq[t], t) for t in k_most_frequent(token_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look for any words that contains non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_tokens = []\n",
    "\n",
    "for token in token_freq.keys():\n",
    "    found_non_alpha = False\n",
    "\n",
    "    for c in token.lower():\n",
    "\n",
    "        if ord(c) < ord('a') or ord(c) > ord('z'):\n",
    "            non_alpha_tokens.append(token)\n",
    "            found_non_alpha = True\n",
    "            continue\n",
    "        \n",
    "        if found_non_alpha:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_alpha_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the most common tokens with non-alpha numeric characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 311503),\n",
       " (',', 354051),\n",
       " ('(', 22524),\n",
       " ('U.S.', 6320),\n",
       " ('30', 2233),\n",
       " ('20', 2259),\n",
       " (\"'t\", 11913),\n",
       " (\"'s\", 70058),\n",
       " ('$', 12917),\n",
       " ('2', 2134),\n",
       " ('``', 90259),\n",
       " (':', 13597),\n",
       " ('2008', 2608),\n",
       " (\"'\", 9576),\n",
       " ('Mr.', 3611),\n",
       " ('&', 2195),\n",
       " ('Â£', 4844),\n",
       " ('--', 22635),\n",
       " (')', 22698),\n",
       " ('1', 2704),\n",
       " ('%', 3194),\n",
       " ('...', 2986),\n",
       " ('2009', 2541),\n",
       " (\"'re\", 2575),\n",
       " ('?', 8331),\n",
       " ('10', 3375),\n",
       " ('!', 2454),\n",
       " ('-', 10053),\n",
       " (';', 5989),\n",
       " ('/', 5138)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_alpha_freq = { t:token_freq[t] for t in non_alpha_tokens }\n",
    "\n",
    "[(t, non_alpha_freq[t]) for t in k_most_frequent(non_alpha_freq, 30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list contains puncuation, years (2006, 2007), word abbreviations (Mr. and U.S.), ending of contractions ('t and 've), and common numbers (2, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
