{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations of Words and Phrases and Their Compositionality\n",
    "\n",
    "This notebook contains an implementation of the Neural Net described in the paper \"Distributed Representations of Words and Phrases and Their Compositionality\". A copy of the paper along with a summary are available in this directory. This implementation is done using Pytorch.\n",
    "\n",
    "Note that the purpose of this notebook is to explore the data and get a better grounding in some of the implementation details and algorithms needed for pre-processing the data, training the final dataset, and interpretting the results. Please look at *torch_final* for a condensed implementation of these methods, minus some of the exploration and discussion.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The corpus being used for training the neural network is not the same corpus mentioned in the paper (the Google News corpus described in the paper is not publicly available). Instead, there is an alternative corpus being used, which may affect the quality of the word vectors.\n",
    "\n",
    "## Analogies Datasets\n",
    "\n",
    "The paper mentions two datasets of analogies they used to evaluate the word embeddings generated by their network. Both of those analogies datasets are publicly available and available in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brendanmcnamara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../data/language-modeling-benchmark-r13/'\n",
    "path_training_corpus = os.path.join(path_data, 'training-monolingual.tokenized.shuffled')\n",
    "path_holdout_corpus = os.path.join(path_data, 'heldout-monolingual.tokenized.shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring a single file in our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load a single file: 0.57s\n",
      "Total samples in the corpus: 306075\n"
     ]
    }
   ],
   "source": [
    "path_filename = os.path.join(path_training_corpus, os.listdir(path_training_corpus)[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(path_filename) as file:\n",
    "    samples_raw = file.read().split(\"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total time to load a single file: {total_time:0.2f}s\")\n",
    "print(f\"Total samples in the corpus: {len(samples_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the words present in the file:\n",
    "- How many total words are in the file?\n",
    "- How many unique words?\n",
    "- What are the counts for each word?\n",
    "- What are the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Corpus\n",
    "\n",
    "We will use *nltk* to tokenize our corpus. As an additional pre-processing step, we will also force all our text to be lowercase. We will also remove any tokens that contains only punctuation characters. These steps may not be necessary with such a large corpus, but for this example, I do it to reduce our vocabulary size and our overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punc_list = [p for p in punctuation]\n",
    "\n",
    "def is_punctuation_token(token):\n",
    "    \"\"\"\n",
    "    A token is a punctuation token if the characters consist of\n",
    "    only punctuation characters\n",
    "    \"\"\"\n",
    "    return len(token) == len([c for c in token if c in punc_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick Test of Above Functions\n",
    "\n",
    "test1 = is_punctuation_token('.') == True\n",
    "test2 = is_punctuation_token('?)()') == True\n",
    "test3 = is_punctuation_token('...1') == False\n",
    "test4 = is_punctuation_token('abc') == False\n",
    "\n",
    "(test1, test2, test3, test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 84.33s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "samples = []\n",
    "\n",
    "for s in samples_raw:\n",
    "    tokens = [t for t in word_tokenize(s.lower()) if not is_punctuation_token(t)]\n",
    "    if tokens:\n",
    "        samples.append(tokens)\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 2.61s\n",
      "Total unique tokens found: 163351\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "token_freq = {}\n",
    "\n",
    "for s in samples:\n",
    "    for t in s:\n",
    "\n",
    "        if t not in token_freq:\n",
    "            token_freq[t] = 0\n",
    "        token_freq[t] = token_freq[t] + 1\n",
    "        \n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:0.2f}s\")\n",
    "\n",
    "print(f\"Total unique tokens found: {len(token_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_frequent(freq, k):\n",
    "    result = []\n",
    "\n",
    "    # Keep track of the index and count of the least frequent token in the\n",
    "    # list. This is the token that is the next candidate to be removed if\n",
    "    # we find a more frequent token.\n",
    "    token_to_remove_index = None\n",
    "    token_to_remove_count = None\n",
    "\n",
    "    # We will allow the user to enter negative numbers, indicating they\n",
    "    # want the least frequent words instead of the most frequent words.\n",
    "    should_append = lambda c: token_to_remove_count < c if k > 0 else token_to_remove_count > c\n",
    "\n",
    "    k_positive = k if k > 0 else -k\n",
    "\n",
    "    for (j, (token, count)) in enumerate(freq.items()):\n",
    "        # If we do not yet have k most frequent tokens, then we can just\n",
    "        # add this token to the list.\n",
    "        if len(result) < k_positive:\n",
    "            result.append(token)\n",
    "            # \"not should_append(c)\" indicates that the current token should\n",
    "            # be the next thing to remove when a better word comes up.\n",
    "            if token_to_remove_count is None or not should_append(count):\n",
    "                token_to_remove_count = count\n",
    "                token_to_remove_index = len(result) - 1\n",
    "            continue\n",
    "\n",
    "        # Check if this word is occurring more frequently than the least\n",
    "        # frequent token in the list.\n",
    "        if should_append(count):\n",
    "            result[token_to_remove_index] = token\n",
    "            \n",
    "            # Need to search for the next token in the list to remove.\n",
    "            token_to_remove_count = freq[result[0]]\n",
    "            token_to_remove_index = 0\n",
    "            for (i, token) in enumerate(result):\n",
    "                # \"not should_append(c)\" indicates that the current token should\n",
    "                # be the next thing to remove when a better word comes up.\n",
    "                if not should_append(freq[token]):\n",
    "                    token_to_remove_count = freq[token]\n",
    "                    token_to_remove_index = i\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a list of the most common words and the number of times they show up in our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 39685),\n",
       " ('was', 47607),\n",
       " ('at', 38909),\n",
       " ('as', 40322),\n",
       " ('that', 70472),\n",
       " ('said', 43592),\n",
       " ('is', 57582),\n",
       " ('for', 67865),\n",
       " ('the', 416622),\n",
       " ('from', 33096),\n",
       " ('of', 176198),\n",
       " ('and', 163408),\n",
       " ('it', 46179),\n",
       " ('on', 59739),\n",
       " ('with', 46932),\n",
       " ('a', 166195),\n",
       " ('by', 35149),\n",
       " ('in', 150692),\n",
       " ('to', 184921),\n",
       " (\"'s\", 70117)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list mostly contains pronouns, articles, and prepositions, along with some punctuation.\n",
    "\n",
    "And also a list of the least frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demsky', 1),\n",
       " ('cocoon-like', 1),\n",
       " ('5,214', 1),\n",
       " ('permasteelisa', 1),\n",
       " ('fratelli', 1),\n",
       " ('volvos', 1),\n",
       " ('seidel', 1),\n",
       " ('63,291', 1),\n",
       " ('sraya', 1),\n",
       " ('germanys', 1),\n",
       " ('computer-aided', 1),\n",
       " ('amitav', 1),\n",
       " ('3-43', 1),\n",
       " ('moreh', 1),\n",
       " ('2-38', 1),\n",
       " ('d10', 1),\n",
       " ('spinboldak', 1),\n",
       " ('checkpoint-friendly', 1),\n",
       " ('yamal-europe', 1),\n",
       " ('s10', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, token_freq[t]) for t in k_most_frequent(token_freq, -20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list includes long numbers (63,291), names of people (Moreh), and joined words (computer-aided, checkpoint-friendly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look for any words that contains non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_tokens = []\n",
    "\n",
    "for token in token_freq.keys():\n",
    "    found_non_alpha = False\n",
    "\n",
    "    for c in token:\n",
    "\n",
    "        if ord(c) < ord('a') or ord(c) > ord('z'):\n",
    "            found_non_alpha = True\n",
    "            continue\n",
    "        \n",
    "    if found_non_alpha:\n",
    "        non_alpha_tokens.append(token)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.67% of our tokens have characters that are non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "ratio = len(non_alpha_tokens) / float(len(token_freq))\n",
    "print(f\"{ratio*100:0.2f}% of our tokens have characters that are non-alphabetical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the most common tokens with non-alpha numeric characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('15', 1770),\n",
       " (\"'re\", 2576),\n",
       " ('£', 4843),\n",
       " ('2008', 2607),\n",
       " (\"'t\", 11929),\n",
       " (\"'s\", 70117),\n",
       " ('2', 2133),\n",
       " ('u.s.', 6516),\n",
       " ('mr.', 3666),\n",
       " ('10', 3374),\n",
       " ('3', 1707),\n",
       " ('1', 2703),\n",
       " ('12', 1825),\n",
       " ('2007', 1983),\n",
       " ('11', 1800),\n",
       " ('2009', 2540),\n",
       " ('30', 2232),\n",
       " ('20', 2258),\n",
       " ('5', 1672),\n",
       " (\"'ve\", 1933)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_alpha_freq = { t:token_freq[t] for t in non_alpha_tokens }\n",
    "\n",
    "[(t, non_alpha_freq[t]) for t in k_most_frequent(non_alpha_freq, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this list contains years (2006, 2007), word abbreviations (Mr. and U.S.), ending of contractions ('t and 've), and common numbers (2, 10). While some of these may lead to extra noise in the data, that is fine since we have such a large corpus. To get something basic working, I'll leave most of these tokens in the vocabulary. We will do a bit of additional processing of the text before we can start working on the neural network:\n",
    "\n",
    "- Marginalize all tokens that occur with a frequency of <= 5 into a single token\n",
    "- Find a generate common phrases in the text and treat them as their own tokens\n",
    "\n",
    "Both of these steps were mentioned in the paper. As a side note, we will be doing the group of our corpus into phrases before we try to remove low-frequency words, since we may end up grouping some of those low frequency words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Common Phrases\n",
    "\n",
    "We will use the formula mentioned in the text:\n",
    "\n",
    "$$score(w_i, w_j) = \\frac {count(w_i, w_j) - \\delta} {count(w_i) \\times count(w_j)}$$\n",
    "\n",
    "The score between any 2 tokens is computed based on their bigram and unigram counts. We've already calculated the unigram counts of every token when we collected their frequencies. We will also need to compute the bigram counts of every pair of tokens in our vocabulary.\n",
    "\n",
    "The $\\delta$ used is a discounting coefficient to prevent too many phrases consisting of infrequent words from forming.\n",
    "\n",
    "Note that $\\delta$ is a coefficient that will be dependent on the corpus size (not normalized), so a value used on some text examples may not extend well to the general training corpus.\n",
    "\n",
    "We need to also choose a score threshold that indicates 2 tokens are merging. After doing a few passes over the entire corpus, we may end up with phrases of 3+ words. Note that after each pass of the database, we will need to recompute our unigram and bigram counts since the set of tokens changes.\n",
    "\n",
    "We will also keep a mapping of which tokens are combined to process our samples after the phrases have been combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_and_bigram_counts(samples):\n",
    "    \"\"\"\n",
    "    Calculate the unigram and bigram counts of our samples.\n",
    "    \n",
    "    Samples - A list of list of tokens. The our list is a single\n",
    "              sample. Each sample contains a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    unigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "\n",
    "    for s in samples:\n",
    "        # Unigram count calculation\n",
    "        for i in range(len(s)):\n",
    "            unigram = s[i]\n",
    "            if unigram not in unigram_counts:\n",
    "                unigram_counts[unigram] = 0\n",
    "            unigram_counts[unigram] = unigram_counts[unigram] + 1\n",
    "               \n",
    "        # Bigram count calculation\n",
    "        for i in range(0, len(s), 2):\n",
    "            # Quit if we do not have enough tokens to form\n",
    "            # more bigrams.\n",
    "            if i + 1 >= len(s):\n",
    "                break\n",
    "                \n",
    "            bigram = (s[i], s[i+1])\n",
    "\n",
    "            if bigram not in bigram_counts:\n",
    "                bigram_counts[bigram] = 0\n",
    "            bigram_counts[bigram] = bigram_counts[bigram] + 1\n",
    "            \n",
    "    return (unigram_counts, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_score(t1, t2, unigram_count, bigram_count, delta):\n",
    "    \"\"\"\n",
    "    Calculate the score for phrase combining.\n",
    "    \n",
    "    unigram_count - A dictionary mapping tokens in our vocabulary\n",
    "                    to their counts.\n",
    "                    \n",
    "    bigram_count - A dictionary mapping pairs of tokens in our vocab\n",
    "                   to their counts.\n",
    "                   \n",
    "    delta - a coefficient used to adjust the score. Higher delta means we\n",
    "            discount infrequent words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Double check that we are not dividing by 0. This should never\n",
    "    # happen in theory because if a token has 0 unigram count, it\n",
    "    # should not be in our vocab.\n",
    "    if t1 not in unigram_count or t2 not in unigram_count:\n",
    "        return 0\n",
    "    \n",
    "    t1u = unigram_count[t1]\n",
    "    t2u = unigram_count[t2]\n",
    "\n",
    "    if t1u == 0 or t2u == 0:\n",
    "        return 0\n",
    "    \n",
    "    b = bigram_count[(t1, t2)]\n",
    "    \n",
    "    return (b - delta) / (t1u * t2u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_map(unigram_count, bigram_count, delta):\n",
    "    \"\"\"\n",
    "    Takes a list of samples and computes a mapping of bigram phrase scoring.\n",
    "    \n",
    "    samples - A list of list of tokens.\n",
    "    \"\"\"    \n",
    "    return { (t1, t2): phrase_score(t1, t2, unigram_count, bigram_count, delta) for (t1, t2) in bigram_count.keys() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 8.06s to run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('farouq', 'al-qadoumi'), 1.0),\n",
       " (('uag', 'tecos'), 1.0),\n",
       " (('dhia', 'al-kawaz'), 1.0),\n",
       " (('01456', '486358'), 1.0),\n",
       " (('akreos', 'sofport'), 1.0),\n",
       " (('jennet', 'mallow'), 1.0),\n",
       " (('total-immersion', 'how-to-be-a-jack0ff-idiot'), 1.0),\n",
       " (('carloforto', 'isola'), 1.0),\n",
       " (('danuta', 'budorina'), 1.0),\n",
       " (('asawat', 'al-iraq'), 1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "uc, bc = calculate_unigram_and_bigram_counts(samples)\n",
    "score_map = create_score_map(uc, bc, delta=0)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"This took {total_time:0.2f}s to run\")\n",
    "\n",
    "[(b, score_map[b]) for b in k_most_frequent(score_map, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that having a delta of 0 results in a lot of matching for pairs of tokens that only occur once in the text. We will change the delta value to penalize low-frequency words.\n",
    "\n",
    "Let's try a few different delta values to see which gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 / 20 completed\n",
      "02 / 20 completed\n",
      "03 / 20 completed\n",
      "04 / 20 completed\n",
      "05 / 20 completed\n",
      "06 / 20 completed\n",
      "07 / 20 completed\n",
      "08 / 20 completed\n",
      "09 / 20 completed\n",
      "10 / 20 completed\n",
      "11 / 20 completed\n",
      "12 / 20 completed\n",
      "13 / 20 completed\n",
      "14 / 20 completed\n",
      "15 / 20 completed\n",
      "16 / 20 completed\n",
      "17 / 20 completed\n",
      "18 / 20 completed\n",
      "19 / 20 completed\n",
      "20 / 20 completed\n",
      "This took 2.81m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "deltas = np.logspace(-1, 3, 20)\n",
    "\n",
    "freqs = []\n",
    "\n",
    "for (i, delta) in enumerate(deltas):\n",
    "    (uc, bc) = calculate_unigram_and_bigram_counts(samples)\n",
    "    score_map = create_score_map(uc, bc, delta)\n",
    "    freq = k_most_frequent(score_map, 40)\n",
    "    freqs.append([(b, score_map[b]) for b in freq])\n",
    "    print(f\"{i+1:02} / {len(deltas)} completed\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"This took {total_time/60:0.2f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.1\n",
      "[('nickname-less', 'whites-only'), ('01456', '486358'), ('anarchism', 'co-exists'), ('poddala', 'jayantha'), ('landrum', 'papered'), ('3888', 'ecocentric.co.uk'), ('dorks', 'jocks'), ('jennet', 'mallow'), ('vujic', 'valjevo'), ('danuta', 'budorina'), ('nicolay', 'bogachev'), ('sural', 'ncv'), ('dhia', 'al-kawaz'), ('valéry', 'lucentini'), ('carloforto', 'isola'), ('4777', 'www.globalgardens.co.uk'), ('wamidh', 'nadhmi'), ('shantanu', 'narayan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('rías', 'baixas'), ('farouq', 'al-qadoumi'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('0532', '793888'), ('162.42', '0.6963'), ('career-long', '57-yarder'), ('aramide', 'olaniyan'), ('violito', 'payla'), ('natasharichardson', 'lindsaylohan'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('rais', 'yatim'), ('mylonitic', 'graphitic'), ('galp', 'energia'), ('t.mccoy', '1-17'), ('bullfighter', 'escamillo'), ('3-36', 'ma.bennett'), ('uag', 'tecos'), ('greater-spotted', 'woodpeckers'), ('akreos', 'sofport'), ('asawat', 'al-iraq')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.16237767391887217\n",
      "[('nickname-less', 'whites-only'), ('anarchism', 'co-exists'), ('landrum', 'papered'), ('farouq', 'al-qadoumi'), ('vujic', 'valjevo'), ('danuta', 'budorina'), ('3888', 'ecocentric.co.uk'), ('poddala', 'jayantha'), ('asawat', 'al-iraq'), ('dorks', 'jocks'), ('nicolay', 'bogachev'), ('jennet', 'mallow'), ('01456', '486358'), ('natasharichardson', 'lindsaylohan'), ('valéry', 'lucentini'), ('sural', 'ncv'), ('dhia', 'al-kawaz'), ('4777', 'www.globalgardens.co.uk'), ('chelyabinsk', 'nizhny'), ('akreos', 'sofport'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('mandu', 'magandi'), ('carloforto', 'isola'), ('0532', '793888'), ('162.42', '0.6963'), ('career-long', '57-yarder'), ('violito', 'payla'), ('uag', 'tecos'), ('wamidh', 'nadhmi'), ('rías', 'baixas'), ('rais', 'yatim'), ('mylonitic', 'graphitic'), ('shantanu', 'narayan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('t.mccoy', '1-17'), ('bullfighter', 'escamillo'), ('aramide', 'olaniyan'), ('galp', 'energia'), ('3-36', 'ma.bennett'), ('greater-spotted', 'woodpeckers')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.26366508987303583\n",
      "[('vujic', 'valjevo'), ('nickname-less', 'whites-only'), ('anarchism', 'co-exists'), ('asawat', 'al-iraq'), ('jennet', 'mallow'), ('poddala', 'jayantha'), ('uag', 'tecos'), ('dorks', 'jocks'), ('nicolay', 'bogachev'), ('3888', 'ecocentric.co.uk'), ('4777', 'www.globalgardens.co.uk'), ('valéry', 'lucentini'), ('wamidh', 'nadhmi'), ('chelyabinsk', 'nizhny'), ('natasharichardson', 'lindsaylohan'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('dhia', 'al-kawaz'), ('mandu', 'magandi'), ('0532', '793888'), ('01456', '486358'), ('shantanu', 'narayan'), ('162.42', '0.6963'), ('farouq', 'al-qadoumi'), ('landrum', 'papered'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('rías', 'baixas'), ('career-long', '57-yarder'), ('violito', 'payla'), ('danuta', 'budorina'), ('rais', 'yatim'), ('sural', 'ncv'), ('akreos', 'sofport'), ('mylonitic', 'graphitic'), ('t.mccoy', '1-17'), ('aramide', 'olaniyan'), ('3-36', 'ma.bennett'), ('galp', 'energia'), ('bullfighter', 'escamillo'), ('carloforto', 'isola'), ('greater-spotted', 'woodpeckers')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.42813323987193935\n",
      "[('nickname-less', 'whites-only'), ('4777', 'www.globalgardens.co.uk'), ('anarchism', 'co-exists'), ('asawat', 'al-iraq'), ('dhia', 'al-kawaz'), ('poddala', 'jayantha'), ('wamidh', 'nadhmi'), ('01456', '486358'), ('natasharichardson', 'lindsaylohan'), ('dorks', 'jocks'), ('sural', 'ncv'), ('3888', 'ecocentric.co.uk'), ('nicolay', 'bogachev'), ('rías', 'baixas'), ('valéry', 'lucentini'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('landrum', 'papered'), ('farouq', 'al-qadoumi'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('0532', '793888'), ('162.42', '0.6963'), ('vujic', 'valjevo'), ('career-long', '57-yarder'), ('bullfighter', 'escamillo'), ('violito', 'payla'), ('uag', 'tecos'), ('rais', 'yatim'), ('danuta', 'budorina'), ('mylonitic', 'graphitic'), ('jennet', 'mallow'), ('shantanu', 'narayan'), ('aramide', 'olaniyan'), ('t.mccoy', '1-17'), ('3-36', 'ma.bennett'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('greater-spotted', 'woodpeckers'), ('akreos', 'sofport'), ('galp', 'energia'), ('carloforto', 'isola')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 0.6951927961775606\n",
      "[('dorks', 'jocks'), ('nicolay', 'bogachev'), ('sbcglobal.net', 'www.penfieldhouse.com'), ('4777', 'www.globalgardens.co.uk'), ('dhia', 'al-kawaz'), ('valéry', 'lucentini'), ('chelyabinsk', 'nizhny'), ('mandu', 'magandi'), ('wamidh', 'nadhmi'), ('uag', 'tecos'), ('total-immersion', 'how-to-be-a-jack0ff-idiot'), ('nlb', 'skladi'), ('cheon', 'ho-seon'), ('dhondup', 'wangchen'), ('www.videonewswire.com', 'event.asp'), ('anatoliy', 'tymoschuk'), ('idris', 'elba'), ('bacillus', 'thuringiensis'), ('chichén', 'itzá'), ('tarsis', 'kabwegyere'), ('jakup', 'krasniqi'), ('plasmodium', 'falciparum'), ('jakrapob', 'penkair'), ('dhi', 'qar'), ('martiga', 'lohn'), ('diosdado', 'cabello'), ('636-4100', 'bam.org'), ('kimmo', 'timonen'), ('aravind', 'adiga'), ('jair', 'jurrjens'), ('glioblastoma', 'multiforme'), ('asharq', 'al-awsat'), ('anousha', 'sakoui'), ('shaila', 'dewan'), ('tarique', 'ghaffur'), ('al-haram', 'al-sharif'), ('jerel', 'mcneal'), ('ritt', 'bjerregaard'), ('paavo', 'lipponen'), ('jamia', 'millia')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1.1288378916846888\n",
      "[('jamia', 'millia'), ('glioblastoma', 'multiforme'), ('abed', 'rabbo'), ('keyon', 'dooling'), ('diosdado', 'cabello'), ('dhi', 'qar'), ('nlb', 'skladi'), ('al-haram', 'al-sharif'), ('sandip', 'moneea'), ('tarique', 'ghaffur'), ('oyu', 'tolgoi'), ('ondrej', 'pavelec'), ('cheon', 'ho-seon'), ('gazzetta', 'dello'), ('ulan', 'bator'), ('dhondup', 'wangchen'), ('asharq', 'al-awsat'), ('shaila', 'dewan'), ('kimmo', 'timonen'), ('paavo', 'lipponen'), ('anousha', 'sakoui'), ('www.videonewswire.com', 'event.asp'), ('anatoliy', 'tymoschuk'), ('idris', 'elba'), ('aravind', 'adiga'), ('yad', 'vashem'), ('martiga', 'lohn'), ('bacillus', 'thuringiensis'), ('chichén', 'itzá'), ('636-4100', 'bam.org'), ('tarsis', 'kabwegyere'), ('ritt', 'bjerregaard'), ('jakup', 'krasniqi'), ('jerel', 'mcneal'), ('shami', 'chakrabarti'), ('plasmodium', 'falciparum'), ('jakrapob', 'penkair'), ('fait', 'accompli'), ('niclas', 'fasth'), ('jair', 'jurrjens')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1.8329807108324356\n",
      "[('nursultan', 'nazarbayev'), ('vitaly', 'churkin'), ('emmerson', 'mnangagwa'), ('palos', 'verdes'), ('austan', 'goolsbee'), ('kelenna', 'azubuike'), ('fareed', 'zakaria'), ('esa-pekka', 'salonen'), ('younes', 'kaboul'), ('sandip', 'moneea'), ('barisan', 'nasional'), ('zalmay', 'khalilzad'), ('methicillin-resistant', 'staphylococcus'), ('kenji', 'johjima'), ('clermont', 'auvergne'), ('salva', 'kiir'), ('zahi', 'hawass'), ('ulan', 'bator'), ('fait', 'accompli'), ('zooey', 'deschanel'), ('farhatullah', 'babar'), ('sheikha', 'lubna'), ('bronislaw', 'komorowski'), ('aitzaz', 'ahsan'), ('warburg', 'pincus'), ('hiroki', 'kuroda'), ('ruslan', 'fedotenko'), ('ku', 'klux'), ('ondrej', 'pavelec'), ('edin', 'dzeko'), ('jesper', 'parnevik'), ('chone', 'figgins'), ('jaroslav', 'halak'), ('hallowe', \"'en\"), ('oyu', 'tolgoi'), ('anquan', 'boldin'), ('yad', 'vashem'), ('shami', 'chakrabarti'), ('gazzetta', 'dello'), ('niclas', 'fasth')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 2.9763514416313175\n",
      "[('anquan', 'boldin'), ('sal', 'oppenheim'), ('hector', 'sants'), ('corpus', 'christi'), ('antawn', 'jamison'), ('ruslan', 'fedotenko'), ('anders', 'fogh'), ('heikki', 'kovalainen'), ('frank-walter', 'steinmeier'), ('maulana', 'fazlullah'), ('zalmay', 'khalilzad'), ('neelie', 'kroes'), ('gwyneth', 'paltrow'), ('methicillin-resistant', 'staphylococcus'), ('atrial', 'fibrillation'), ('ilya', 'kovalchuk'), ('marat', 'safin'), ('younes', 'kaboul'), ('susilo', 'bambang'), ('hallowe', \"'en\"), ('preah', 'vihear'), ('ku', 'klux'), ('austan', 'goolsbee'), ('hiroki', 'kuroda'), ('virender', 'sehwag'), ('phnom', 'penh'), ('amare', 'stoudemire'), ('ulan', 'bator'), ('tal', 'afar'), ('chone', 'figgins'), ('flavio', 'briatore'), ('izabella', 'kaminska'), ('dunkin', 'donuts'), ('crédit', 'agricole'), ('hedo', 'turkoglu'), ('warburg', 'pincus'), ('burkina', 'faso'), ('société', 'générale'), ('barisan', 'nasional'), ('sumitomo', 'mitsui')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 4.832930238571752\n",
      "[('yar', \"'adua\"), ('corpus', 'christi'), ('ku', 'klux'), ('luiz', 'inacio'), ('amare', 'stoudemire'), ('slobodan', 'milosevic'), ('dera', 'ismail'), ('kung', 'fu'), ('timo', 'glock'), ('tal', 'afar'), ('uttar', 'pradesh'), ('izabella', 'kaminska'), ('neelie', 'kroes'), ('younes', 'kaboul'), ('preah', 'vihear'), ('susilo', 'bambang'), ('kaiser', 'permanente'), ('lamar', 'odom'), ('northrop', 'grumman'), ('société', 'générale'), ('pau', 'gasol'), ('khmer', 'rouge'), ('heikki', 'kovalainen'), ('flavio', 'briatore'), ('recep', 'tayyip'), ('buenos', 'aires'), ('gwyneth', 'paltrow'), ('sergey', 'brin'), ('otc', 'bulletin'), ('burkina', 'faso'), ('mamma', 'mia'), ('wes', 'streeting'), ('kuala', 'lumpur'), ('frank-walter', 'steinmeier'), ('hedo', 'turkoglu'), ('phnom', 'penh'), ('terra', 'firma'), ('lashkar', 'gah'), ('meryl', 'streep'), ('dunkin', 'donuts')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 7.847599703514611\n",
      "[('sao', 'paulo'), ('northrop', 'grumman'), ('costa', 'rica'), ('kaiser', 'permanente'), ('quentin', 'tarantino'), ('tel', 'aviv'), ('hedo', 'turkoglu'), ('shi', \"'ite\"), ('pau', 'gasol'), ('kuala', 'lumpur'), ('wen', 'jiabao'), ('luiz', 'inacio'), ('padraig', 'harrington'), ('sergey', 'brin'), ('recep', 'tayyip'), ('dunkin', 'donuts'), ('roland', 'garros'), ('flavio', 'briatore'), ('meryl', 'streep'), ('lashkar', 'gah'), ('sinn', 'fein'), ('gon', 'na'), ('thaksin', 'shinawatra'), ('lamar', 'odom'), ('exxon', 'mobil'), ('umar', 'farouk'), ('notre', 'dame'), ('tayyip', 'erdogan'), ('cerebral', 'palsy'), ('slobodan', 'milosevic'), ('gwyneth', 'paltrow'), ('izabella', 'kaminska'), ('buenos', 'aires'), ('suu', 'kyi'), ('otc', 'bulletin'), ('angelina', 'jolie'), ('kung', 'fu'), ('khmer', 'rouge'), ('terra', 'firma'), ('dirk', 'nowitzki')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 12.742749857031335\n",
      "[('gon', 'na'), ('sri', 'lanka'), ('arnold', 'schwarzenegger'), ('â', '®'), ('abu', 'dhabi'), ('dmitry', 'medvedev'), ('benedict', 'xvi'), ('goldman', 'sachs'), ('benazir', 'bhutto'), ('britney', 'spears'), ('roland', 'garros'), ('las', 'vegas'), ('fannie', 'mae'), ('angela', 'merkel'), ('notre', 'dame'), ('l', \"'aquila\"), ('buenos', 'aires'), ('padraig', 'harrington'), ('fabio', 'capello'), ('exxon', 'mobil'), ('differ', 'materially'), ('lib', 'dems'), ('otc', 'bulletin'), ('sinn', 'fein'), ('alistair', 'darling'), ('shi', \"'ite\"), ('cristiano', 'ronaldo'), ('des', 'moines'), ('tel', 'aviv'), ('hu', 'jintao'), ('osama', 'bin'), ('dalai', 'lama'), ('thaksin', 'shinawatra'), ('khmer', 'rouge'), ('suu', 'kyi'), ('kuala', 'lumpur'), ('puerto', 'rico'), ('hamid', 'karzai'), ('costa', 'rica'), ('‚', 'â')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 20.6913808111479\n",
      "[('las', 'vegas'), ('goldman', 'sachs'), ('dalai', 'lama'), ('notre', 'dame'), ('nicolas', 'sarkozy'), ('puerto', 'rico'), ('freddie', 'mac'), ('hong', 'kong'), ('walt', 'disney'), ('fannie', 'mae'), ('suu', 'kyi'), ('buenos', 'aires'), ('gon', 'na'), ('sri', 'lanka'), ('merrill', 'lynch'), ('hamid', 'karzai'), ('swine', 'flu'), ('abu', 'dhabi'), ('quote', 'profile'), ('differ', 'materially'), ('nancy', 'pelosi'), ('sri', 'lankan'), ('greenhouse', 'gases'), ('wells', 'fargo'), ('arnold', 'schwarzenegger'), ('iron', 'ore'), ('ac', 'milan'), ('benazir', 'bhutto'), ('saudi', 'arabia'), ('â', '®'), ('angela', 'merkel'), ('bin', 'laden'), ('bear', 'stearns'), ('dmitry', 'medvedev'), ('alistair', 'darling'), ('osama', 'bin'), ('mitt', 'romney'), ('tel', 'aviv'), ('saddam', 'hussein'), ('carbon', 'dioxide')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 33.59818286283781\n",
      "[('nicolas', 'sarkozy'), ('greenhouse', 'gases'), ('merrill', 'lynch'), ('nancy', 'pelosi'), ('hamid', 'karzai'), ('grand', 'slam'), ('osama', 'bin'), ('quote', 'profile'), ('sri', 'lanka'), ('abu', 'dhabi'), ('swine', 'flu'), ('super', 'bowl'), ('fannie', 'mae'), ('capitol', 'hill'), ('goldman', 'sachs'), ('alistair', 'darling'), ('bin', 'laden'), ('morgan', 'stanley'), ('tampa', 'bay'), ('dalai', 'lama'), ('forward-looking', 'statements'), ('notre', 'dame'), ('suu', 'kyi'), ('al', 'qaeda'), ('pleaded', 'guilty'), ('los', 'angeles'), ('lehman', 'brothers'), ('carbon', 'dioxide'), ('hillary', 'rodham'), ('freddie', 'mac'), ('�', '�'), ('hong', 'kong'), ('san', 'francisco'), ('san', 'diego'), ('arnold', 'schwarzenegger'), ('saudi', 'arabia'), ('st.', 'louis'), ('bear', 'stearns'), ('las', 'vegas'), ('sarah', 'palin')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 54.555947811685144\n",
      "[('quote', 'profile'), ('pleaded', 'guilty'), ('w.', 'bush'), ('las', 'vegas'), ('sarah', 'palin'), ('dalai', 'lama'), ('bin', 'laden'), ('forward-looking', 'statements'), ('human', 'rights'), ('san', 'francisco'), ('saudi', 'arabia'), ('hamid', 'karzai'), ('premier', 'league'), ('super', 'bowl'), ('abu', 'dhabi'), ('los', 'angeles'), ('san', 'diego'), ('fannie', 'mae'), ('real', 'estate'), ('wall', 'street'), ('morgan', 'stanley'), ('carbon', 'dioxide'), ('per', 'cent'), ('�', '�'), ('northern', 'ireland'), ('web', 'site'), ('st.', 'louis'), ('tampa', 'bay'), ('prime', 'minister'), ('goldman', 'sachs'), ('hillary', 'rodham'), ('global', 'warming'), ('sri', 'lanka'), ('rodham', 'clinton'), ('hong', 'kong'), ('al', 'qaeda'), ('gordon', 'brown'), ('associated', 'press'), ('swine', 'flu'), ('george', 'w.')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 88.58667904100822\n",
      "[('associated', 'press'), ('web', 'site'), ('barack', 'obama'), ('federal', 'reserve'), ('wall', 'street'), ('global', 'warming'), ('middle', 'east'), ('gordon', 'brown'), ('forward-looking', 'statements'), ('health', 'care'), ('hong', 'kong'), ('profile', 'research'), ('san', 'diego'), ('las', 'vegas'), ('climate', 'change'), ('south', 'africa'), ('northern', 'ireland'), ('�', '�'), ('click', 'here'), ('george', 'w.'), ('w.', 'bush'), ('vice', 'president'), ('north', 'korea'), ('prime', 'minister'), ('premier', 'league'), ('united', 'states'), ('supreme', 'court'), ('los', 'angeles'), ('fourth', 'quarter'), ('human', 'rights'), ('al', 'qaeda'), ('white', 'house'), ('real', 'estate'), ('chief', 'executive'), ('san', 'francisco'), ('quote', 'profile'), ('interest', 'rates'), ('per', 'cent'), ('saudi', 'arabia'), ('swine', 'flu')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 143.8449888287663\n",
      "[('wall', 'street'), ('barack', 'obama'), ('interest', 'rates'), ('years', 'ago'), ('federal', 'reserve'), ('real', 'estate'), ('didn', \"'t\"), ('north', 'korea'), ('hong', 'kong'), ('washington', 'ap'), ('las', 'vegas'), ('doesn', \"'t\"), ('per', 'cent'), ('i', 'am'), ('isn', \"'t\"), ('san', 'francisco'), ('wouldn', \"'t\"), ('chief', 'executive'), ('web', 'site'), ('human', 'rights'), ('associated', 'press'), ('health', 'care'), ('san', 'diego'), ('south', 'africa'), ('premier', 'league'), ('los', 'angeles'), ('don', \"'t\"), ('supreme', 'court'), ('i', \"'m\"), ('rather', 'than'), ('john', 'mccain'), ('wasn', \"'t\"), ('white', 'house'), ('new', 'york'), ('vice', 'president'), ('climate', 'change'), ('gordon', 'brown'), ('�', '�'), ('prime', 'minister'), ('united', 'states')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 233.57214690901213\n",
      "[('i', 'think'), ('don', \"'t\"), ('per', 'cent'), ('isn', \"'t\"), ('more', 'than'), ('last', 'week'), ('washington', 'ap'), ('i', \"'m\"), ('prime', 'minister'), ('web', 'site'), ('i', \"'ve\"), ('associated', 'press'), ('did', 'not'), ('no', 'longer'), ('last', 'month'), ('san', 'francisco'), ('at', 'least'), ('years', 'ago'), ('wall', 'street'), ('we', \"'re\"), ('wasn', \"'t\"), ('barack', 'obama'), ('last', 'year'), ('los', 'angeles'), ('new', 'york'), ('rather', 'than'), ('human', 'rights'), ('so', 'far'), ('white', 'house'), ('i', 'don'), ('gordon', 'brown'), ('health', 'care'), ('chief', 'executive'), ('united', 'states'), ('less', 'than'), ('doesn', \"'t\"), ('climate', 'change'), ('i', 'am'), ('north', 'korea'), ('didn', \"'t\")]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 379.2690190732246\n",
      "[('i', 'think'), ('last', 'year'), ('i', \"'m\"), ('as', 'well'), ('this', 'week'), ('last', 'week'), ('last', 'month'), ('i', 'don'), ('don', \"'t\"), ('white', 'house'), ('will', 'be'), ('at', 'least'), ('this', 'year'), ('health', 'care'), ('did', 'not'), ('according', 'to'), ('years', 'ago'), ('you', 'can'), ('we', \"'re\"), ('new', 'york'), ('i', \"'ve\"), ('so', 'far'), ('had', 'been'), ('united', 'states'), ('such', 'as'), ('didn', \"'t\"), ('i', 'am'), ('has', 'been'), ('rather', 'than'), ('barack', 'obama'), ('more', 'than'), ('there', 'are'), ('prime', 'minister'), ('if', 'you'), ('doesn', \"'t\"), ('would', 'be'), ('los', 'angeles'), ('per', 'cent'), ('have', 'been'), ('chief', 'executive')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 615.8482110660261\n",
      "[('i', 'think'), ('did', 'not'), ('there', 'is'), ('you', 'can'), ('according', 'to'), ('last', 'week'), ('it', \"'s\"), ('could', 'be'), ('had', 'been'), ('i', \"'m\"), ('los', 'angeles'), ('it', 'is'), ('prime', 'minister'), ('have', 'been'), ('there', 'are'), ('last', 'year'), ('we', 'are'), ('this', 'year'), ('i', 'am'), ('more', 'than'), ('this', 'week'), ('new', 'york'), ('one', 'of'), ('he', 'was'), ('will', 'be'), ('per', 'cent'), ('part', 'of'), ('don', \"'t\"), ('has', 'been'), ('they', 'are'), ('would', 'be'), ('at', 'least'), ('he', 'said'), ('as', 'well'), ('such', 'as'), ('didn', \"'t\"), ('they', 'were'), ('united', 'states'), ('it', 'was'), ('if', 'you')]\n",
      "\n",
      "\n",
      "\n",
      "Delta: 1000.0\n",
      "[('it', 'is'), ('the', 'first'), ('at', 'least'), ('they', 'are'), ('had', 'been'), ('per', 'cent'), ('of', 'the'), ('will', 'be'), ('in', 'the'), ('this', 'is'), ('it', \"'s\"), ('he', 'said'), ('the', 'same'), ('would', 'be'), ('such', 'as'), ('this', 'year'), ('he', 'was'), ('i', \"'m\"), ('part', 'of'), ('the', 'united'), ('there', 'are'), ('one', 'of'), ('more', 'than'), ('don', \"'t\"), ('there', 'is'), ('united', 'states'), ('to', 'make'), ('the', 'world'), ('as', 'well'), ('to', 'be'), ('new', 'york'), ('as', 'a'), ('last', 'year'), ('the', 'company'), ('they', 'were'), ('according', 'to'), ('has', 'been'), ('at', 'the'), ('it', 'was'), ('have', 'been')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(deltas)):\n",
    "    print(f\"Delta: {deltas[i]}\")\n",
    "    print([b for b, _ in freqs[i]])\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that low delta values favor very rare words but very high delta values favor very common words. This is likely because for high delta values, the most important thing is our bigram count is larger than delta. For a delta of 1000, anything that has a bigram count of less than 1000 will automatically get scored less than bigrams with a count of greater than 1000. In this way, you can think of the delta score as thresholding bigram counts that we care about.\n",
    "\n",
    "With this in mind, let's look at some statistics of bigram counts in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bigram_counts_map = calculate_unigram_and_bigram_counts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [f + \" \" + s for f,s in bigram_counts_map.keys()]\n",
    "vals = list(bigram_counts_map.values())\n",
    "bigram_series = pd.Series(index=keys, data=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.164472e+06\n",
       "mean     2.887178e+00\n",
       "std      3.837628e+01\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      1.000000e+00\n",
       "75%      1.000000e+00\n",
       "max      2.073000e+04\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our statistics that the vast majority of bigrams frequencies of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7545840518277812"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_series[bigram_series == 1]) / len(bigram_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick search over quantiles of our series to get a better understanding of how our data is spread out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9, 3.0),\n",
       " (0.99, 28.0),\n",
       " (0.999, 179.0),\n",
       " (0.9999, 917.0),\n",
       " (0.99999, 3934.423200007528),\n",
       " (0.999999, 18025.34677637741),\n",
       " (0.9999999, 20636.14363762783),\n",
       " (0.99999999, 20720.614363668952),\n",
       " (0.999999999, 20729.06143634813),\n",
       " (0.9999999999, 20729.906143540982)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(q, bigram_series.quantile(q)) for q in 1 - np.logspace(-1, -10, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will pick our $\\delta$ to be the 99th quantile of bigram counts, which will automatically give below-zero scores to any bigram that occurs less frequently than the top 99 percent of bigrams in our corpus. This may not be the best way to choose $\\delta$, but it will give us a corpus-size agnostic way of doing so.\n",
    "\n",
    "We then need to choose a valid score threshold, above which are bigrams we consider to be valid phrases. Let's take a look at what scores look like on our current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = bigram_series.quantile(.99)\n",
    "uc, bc = calculate_unigram_and_bigram_counts(samples)\n",
    "score_map = create_score_map(uc, bc, delta)\n",
    "score_series = pd.Series(index=list(score_map.keys()), data=list(score_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.164472e+06\n",
       "mean    -1.004987e-01\n",
       "std      1.268294e+00\n",
       "min     -2.700000e+01\n",
       "25%     -5.974870e-04\n",
       "50%     -5.144288e-05\n",
       "75%     -6.406394e-06\n",
       "max      3.155680e-03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, since we set delta to the 99% quantile, we should expect 99% of our scores less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897103579991618"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_series[score_series < 0]) / len(score_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach would be to create phrases from bigram scores greater than 0. In this case, we would only be considering the bigram counts, since, in our equation above, dividing by the product of the unigram counts would never change the sign of our score.\n",
    "\n",
    "Let's take a look at just our positive scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score_series = score_series[score_series > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.151100e+04\n",
       "mean     5.906361e-06\n",
       "std      6.370966e-05\n",
       "min      2.192717e-10\n",
       "25%      6.156941e-08\n",
       "50%      1.915316e-07\n",
       "75%      6.312957e-07\n",
       "max      3.155680e-03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_score_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(suu, kyi)             0.003156\n",
       "(dalai, lama)          0.002205\n",
       "(notre, dame)          0.002037\n",
       "(fannie, mae)          0.001528\n",
       "(las, vegas)           0.001422\n",
       "(sri, lanka)           0.001368\n",
       "(abu, dhabi)           0.001323\n",
       "(hong, kong)           0.001292\n",
       "(alistair, darling)    0.001256\n",
       "(goldman, sachs)       0.001128\n",
       "(saudi, arabia)        0.001113\n",
       "(hamid, karzai)        0.000996\n",
       "(bin, laden)           0.000912\n",
       "(nancy, pelosi)        0.000883\n",
       "(freddie, mac)         0.000798\n",
       "(merrill, lynch)       0.000777\n",
       "(carbon, dioxide)      0.000741\n",
       "(swine, flu)           0.000699\n",
       "(osama, bin)           0.000655\n",
       "(quote, profile)       0.000647\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_score_series.nlargest(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(this, the)      2.192717e-10\n",
       "(of, not)        2.363976e-10\n",
       "(the, may)       3.822674e-10\n",
       "(her, the)       3.854597e-10\n",
       "(to, at)         4.169509e-10\n",
       "(this, to)       4.940131e-10\n",
       "(that, but)      5.120168e-10\n",
       "(in, by)         5.663933e-10\n",
       "(that, that)     6.040711e-10\n",
       "(which, to)      6.446197e-10\n",
       "('s, on)         7.162093e-10\n",
       "(them, of)       7.731145e-10\n",
       "('s, that)       8.095060e-10\n",
       "(people, the)    8.722658e-10\n",
       "(to, and)        9.266132e-10\n",
       "(to, where)      9.463974e-10\n",
       "(do, and)        1.004539e-09\n",
       "(you, in)        1.013679e-09\n",
       "(last, of)       1.059640e-09\n",
       "(the, 20)        1.063001e-09\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_score_series.nsmallest(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from our low-scoring examples, they are being penalized for using common words, and therefore have higher unigram counts.\n",
    "\n",
    "Among the positive scores, we will threshold our phrasing at the top 97% of scores. This value is empirically chosen, and more could be done to search for better score thresholding.\n",
    "\n",
    "We will consolidate the above procedure for combining common phrases into a single algorithm.\n",
    "\n",
    "*NOTE: We will use a greedy implementation for combining phrases. If for example, we had a triplet of tokens (a,b,c) such that (b,c) had a higher score than (a,b), we will end up combining (a,b) simply because we will see it first as we enumerate our samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bigrams(samples, bigrams):\n",
    "    \"\"\"\n",
    "    Given a list of tokenized samples, create a new list of samples where bigrams\n",
    "    have been merged.\n",
    "    \n",
    "    samples - A list of samples, each sample being a list of tokens.\n",
    "    bigrams - A set of bigrams to merge.\n",
    "    \"\"\"\n",
    "    new_samples = []\n",
    "\n",
    "    for sample in samples:\n",
    "        \n",
    "        if len(sample) == 0:\n",
    "            print(\"WARNING SAMPLE LEN IS 0\", not sample)\n",
    "\n",
    "        new_sample = []\n",
    "\n",
    "        # Keep track if we merge in the previous iteration so we don't\n",
    "        # merge overlapping phrases: for (a, b, c), if (a, b) was merged\n",
    "        # we do not want to merge (b, c).\n",
    "        merged_during_previous_iter = False\n",
    "\n",
    "        for i in range(len(sample) - 1):\n",
    "            if merged_during_previous_iter:\n",
    "                merged_during_previous_iter = False\n",
    "                continue\n",
    "            \n",
    "            current = (sample[i], sample[i+1])\n",
    "            if current in bigrams:\n",
    "                new_sample.append(sample[i] + \" \" + sample[i + 1])\n",
    "                merged_during_previous_iter = True\n",
    "            else:\n",
    "                new_sample.append(sample[i])\n",
    "                \n",
    "        # We do not iterate the last element. So if the last pair was not\n",
    "        # merged, we need to add back the last token.\n",
    "        if not merged_during_previous_iter:\n",
    "            new_sample.append(sample[-1])\n",
    "\n",
    "        new_samples.append(new_sample)\n",
    "                \n",
    "    return new_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['a', 'b', 'c']], [['a', 'b c']], [['a b', 'c']])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the above merge operation.\n",
    "\n",
    "result1 = merge_bigrams([['a', 'b', 'c']], bigrams={})\n",
    "result2 = merge_bigrams([['a', 'b', 'c']], bigrams={('b', 'c')})\n",
    "result3 = merge_bigrams([['a', 'b', 'c']], bigrams={('a', 'b'), ('b', 'c')})\n",
    "\n",
    "(result1, result2, result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_common_phrases(samples, bigram_percentile=0.95, score_percentile=0.999):\n",
    "    uc, bc = calculate_unigram_and_bigram_counts(samples)\n",
    "    \n",
    "    # Figure out a good delta value using bigram quantile\n",
    "    bc_series = pd.Series(data=list(bc.values()))\n",
    "    delta = bc_series.quantile(bigram_percentile)\n",
    "    \n",
    "    # Calculate score map and threshold\n",
    "    score_map = create_score_map(uc, bc, delta)\n",
    "    score_series = pd.Series(data=list(score_map.values()))\n",
    "    score_threshold = score_series.quantile(score_percentile).item()\n",
    "    \n",
    "    # Find the phrases that have a high-enough score and generate\n",
    "    # a new set of samples with those phrases merged into a single\n",
    "    # token.\n",
    "    phrases = {b for b, s in score_map.items() if s > score_threshold}\n",
    "    return merge_bigrams(samples, phrases), phrases\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do 4 full passes over our samples to generate new common phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\tIter Time: 17.19s\tTotal Time: 17.19s\tTokens: 164422\tNew Phrases: 1165\n",
      "Iter: 2\tIter Time: 16.49s\tTotal Time: 33.68s\tTokens: 165519\tNew Phrases: 1173\n",
      "Iter: 3\tIter Time: 17.38s\tTotal Time: 51.05s\tTokens: 166654\tNew Phrases: 1183\n",
      "Iter: 4\tIter Time: 17.28s\tTotal Time: 68.33s\tTokens: 167826\tNew Phrases: 1195\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iter_time = start_time\n",
    "\n",
    "token_counts_per_iter = []\n",
    "phrases_per_iter = []\n",
    "\n",
    "combined_samples = samples\n",
    "combined_token_freq = token_freq\n",
    "\n",
    "for i in range(4):\n",
    "    combined_samples, phrases = combine_common_phrases(combined_samples)\n",
    "    uc, _ = calculate_unigram_and_bigram_counts(combined_samples)\n",
    "\n",
    "    combined_token_freq = uc\n",
    "\n",
    "    token_counts_per_iter.append(len(uc))\n",
    "    phrases_per_iter.append(phrases)\n",
    "\n",
    "    prev_iter_time = iter_time\n",
    "    iter_time = time.time()\n",
    "\n",
    "    print(f\"Iter: {i+1}\\tIter Time: {iter_time-prev_iter_time:.2f}s\\tTotal Time: {iter_time - start_time:.2f}s\\tTokens: {len(uc)}\\tNew Phrases: {len(phrases)}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use a special token to marginalize over any words that occur less than 5 times. Let's pick a low count token key that is not being used anywhere else in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_TOKEN_COUNT = 6\n",
    "LOW_COUNT_TOKEN = '__LOW_COUNT_TOKEN__'\n",
    "LOW_COUNT_TOKEN in combined_token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126937"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_count_tokens = {t for t,c in combined_token_freq.items() if c < MIN_TOKEN_COUNT}\n",
    "len(low_count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_token_freq = {}\n",
    "\n",
    "for token, count in combined_token_freq.items():\n",
    "    if count < MIN_TOKEN_COUNT:\n",
    "        prev_count = final_token_freq[LOW_COUNT_TOKEN] if LOW_COUNT_TOKEN in final_token_freq else 0\n",
    "        final_token_freq[LOW_COUNT_TOKEN] = prev_count + count\n",
    "    else:\n",
    "        final_token_freq[token] = count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_samples = []\n",
    "\n",
    "for sample in samples:\n",
    "    final_sample = []\n",
    "\n",
    "    for t in sample:\n",
    "        if t in low_count_tokens:\n",
    "            final_sample.append(LOW_COUNT_TOKEN)\n",
    "        else:\n",
    "            final_sample.append(t)\n",
    "            \n",
    "    final_samples.append(final_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Tokens\n",
    "\n",
    "Before we can feed our data into a neural network, we need to encode our samples using one-hot encoding. First, we need to map every token in our vocabulary to an integer, which we can then use to help us encode the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes token to index\n",
    "encoder = { t:i for i,t in enumerate(final_token_freq.keys()) }\n",
    "\n",
    "# Decodes index to token\n",
    "decoder = { t:i for i,t in encoder.items() }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an encoder / decoder, we need to use those for encoding / decoding the tokens to / from one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vectors(tokens, encoder):\n",
    "    \"\"\"\n",
    "    Outputs set of one-hot vectors. The output is a numpy 2D array,\n",
    "    with dimension (len(tokens), V), where V is size of vocab. So each\n",
    "    column is a one-hot vector for each token in the list.\n",
    "    \n",
    "    tokens - A list of string tokens to encode\n",
    "    encoder - A token to index mapping for each token in the vocab\n",
    "    \"\"\"\n",
    "    vocab_size = len(encoder)\n",
    "    encoded = np.zeros((len(tokens), vocab_size))\n",
    "    token_map = [encoder[t] for t in tokens]\n",
    "\n",
    "    encoded[np.arange(len(tokens)), token_map] = 1.0\n",
    "    \n",
    "    return encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to make sure above code is correct\n",
    "test_encoder = {'a': 0, 'b': 1, 'c': 2}\n",
    "create_one_hot_vectors(['a', 'c', 'a', 'b'], test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(encoded, decoder):\n",
    "    \"\"\"\n",
    "    Converts a set of one-hot encoded vectors into a list of their\n",
    "    corresponding tokens. This outpus a list of string tokens.\n",
    "    \n",
    "    encoded - a numpy array of one-hot encodings, where each column\n",
    "              is a one-hot encoded vector of a token, and each row\n",
    "              indexes into the vocabulary.\n",
    "              \n",
    "    decoder - a mapping from int to string token, used for decoding\n",
    "              the indices of one-hot vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [decoder[i] for i in np.argmax(encoded, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c', 'a', 'b']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to make sure above code is correct.\n",
    "test_encoder = {'a': 0, 'b': 1, 'c': 2}\n",
    "test_decoder = {0: 'a', 1: 'b', 2: 'c'}\n",
    "\n",
    "test_encoding = create_one_hot_vectors(['a', 'c', 'a', 'b'], test_encoder)\n",
    "create_tokens(test_encoding, test_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
