# A Neural Probabilistic Language Model

## What are the motivations for this work?

The main problem ("people problem") that this paper is addressing is the need for better language models. The
motivation of creating a better language model comes from the need to use these model to solve real-word AI problems
in natural language such as speech-to-text transcription, machine translation, and semantic analysis. Specifically, this
paper investigates statistical language models, which are models that try to generate probabilities of words given
a set or sequence of words. The authors do not seriously explore non-statistical language models though there are a few
of these types of models mentioned in the section *relation to previous work*.

The main technical problem
is improving the quality of statistical language models by addressing the *"curse of dimensionality"*, a common problem
in statistical learning that makes finding generalized algorithms in high-dimensional spaces hard to develop. In particular,
this curse of dimensionality comes up when training a statistical model to predict long sequences of words. As a statistical
model conditions on a longer sequence, the model has more information to make predictions, but the effects of
dimensionality negatively impact the performance of these more complex models.

## What is the proposed solution?

The proposed solution is to use a multi-layer neural network to train a better-generalizing statistical model. The hidden
layers of the network will find vector representations of words such that semantically similar words are closer to one
another in the cartesian space. The hypothesis is that these word vector representations along with the statistical
model can be learned simultaneously. Having these word vector representations in a continuous space achieves generalization
as similar words cluster closer to one another.

## What is the works evaluation of the proposed solution?

The authors compare the new statistical language model generated by training a neural network to the existing state-of-the-art
statistical language models. The comparison metric for these models is *perplexity* on the hold-out / test set. Perplexity is the most
common application-agnostic measurements for the quality of a statistical language model (one can think of perplexity as a
non-linear proxy for innacuracy). The statistical model with the lowest perplexity on the hold-out set is considered the best.

## What is your analysis of the identified problem, idea, and evaluation?

I think this new approach is a great idea. In addition to providing a potentially better langauge model, the training
of the neural network generates these word vector representations which could be used to capture semantics of a set of
words. The word vectors could be transfered to other applications in natural language processing. The two main points of
concern I have are:

1. Training such a model is much more computationally demanding than other statistical language models
2. There are some fundamental questions of what information the word vectors are actually capturing. Because they are
   being used to model word sequences, does that make them more bias to capturing some application-specific
   features such as grammatical structures? How well would these word vectors generalize to other natural language problems?
   Finally, how much information is lost when breaking down the content of a corpus into individual words when natural
   language is so context-heavy and idiomatic?

## What are the contributions?

1. Creating a statistical language model that does a better job with generalization
2. Providing a training mechanism for such a model that (even though it is much more computationally expensive than previous
   models), scales linearly with vocabulary size and linearly with context size.
3. Implementing 2 alternative parallel algorithms for training the model: one that utilizes training the model on a single
   machine (and parallelizes across the data) and the other that utilizes training the model on a cluser of machines (and
   parallelizes across parameters in the model).
   
## What are the future directions for this research?

Future research directions have been listed towards the end of the paper. To summarize, these fall under the categories of:
(1) finding ways to make training more computationally efficient, (2) applying the word vector representations to other
natural language applications, and (3) improving the overall quality of the word vectors.

One idea not mentioned:
is there a good way to add common idioms to the vocabulary captured by the word vectors in addition to the individual
words found in the corpus? Idioms can capture meaning that is very different than the meaning of the individual words.

## What questions are left with you?

This paper introduces the idea of using word vectors in a continuous space to provide more generalized statistical language
models, and hints at the fact that these word vector representations can be used in other applications of natural language
understanding, but does not explore this idea too deeply. My main question is how do the word vector representations get
applied to other applications and how do they compare to the alternatives (one-hot / orthogonal vectors of words, or
hand-picked features from words)?

Instead of the author only comparing their neural network to n-gram models, it would be interesting to compare their
neural networks to other neural networks where features are captured in different ways.

## What is the takeaway message for this paper?

Learning a distributed vector representation of words will lead to a more generalizable mechanism for
statistical language models. Having this distributed representation gives us a mechanism of learning the likelihood of many
sequences from a single sequence by generalizing across a continuous space. Because features / dimension of these word vectors are
not hand-picked but rather discovered entirely by optimizing a language model, they can provide more powerful ways of capturing
semantic information in language and using that information for other applications.
